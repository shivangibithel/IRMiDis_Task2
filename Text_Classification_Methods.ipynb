{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "janWv1vG5xUD",
        "bQGOVFFwaepj",
        "fOKz8xQr5xXJ",
        "BscPlX2tZOPS",
        "fBoBD2BsZVUW",
        "qYP5Sg-VaqEi",
        "vs7PJfyRb5dH",
        "bpAWfjkCl7oH",
        "sR62UJuynCDW",
        "8qY2Ug9KyTqN",
        "mQFa-Q54yNxa",
        "TcMKcvRgaLOK",
        "lKaOn7EUDAYi",
        "gWPOVg0pDMTK",
        "odFGh1iEFgP4",
        "w7TuyLjUGp3H",
        "Ka8GM_FdKdyu",
        "oU5QeBxTLccA",
        "eInI6X92DFhR"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivangibithel/IRMiDis_Task2/blob/main/Text_Classification_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27hY5InNb60"
      },
      "source": [
        "[A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janWv1vG5xUD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PV3HrLnLJAY"
      },
      "source": [
        "# pip install emoji --upgrade"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvvarqE5xWm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd #to work with csv files\n",
        "\n",
        "#matplotlib imports are used to plot confusion matrices for the classifiers\n",
        "import matplotlib as mpl \n",
        "import matplotlib.cm as cm \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#import feature extraction methods from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "\n",
        "#pre-processing of text\n",
        "import string\n",
        "import re\n",
        "\n",
        "#import classifiers from sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#import different metrics to evaluate the classifiers\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn import metrics\n",
        "\n",
        "#import time function from time module to track the training duration\n",
        "from time import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFWAClI3mkMK",
        "outputId": "53a438ce-1f62-4752-9c6a-622a58415a05"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGOVFFwaepj"
      },
      "source": [
        "# Section 1: Load and explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iPh67KcCh5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "0adb0419-7e32-4448-b178-be54bd389bdf"
      },
      "source": [
        "our_data = pd.read_csv(\"cleaned_train.csv\",index_col = \"id\")\n",
        "test_data = pd.read_csv(\"cleaned_test.csv\",index_col = \"id\")\n",
        "our_data = shuffle(our_data)\n",
        "our_data.head()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.330830e+18</th>\n",
              "      <td>a quick reminder that cambridge not known for ...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325910e+18</th>\n",
              "      <td>really good news pfizer and biontech report th...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326200e+18</th>\n",
              "      <td>nhs to be ready to roll out coronavirus vaccin...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329030e+18</th>\n",
              "      <td>so its gone up just as a new vaccine announced...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.332280e+18</th>\n",
              "      <td>south korea says it foiled north korea attempt...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet  ... label_num\n",
              "id                                                               ...          \n",
              "1.330830e+18  a quick reminder that cambridge not known for ...  ...         2\n",
              "1.325910e+18  really good news pfizer and biontech report th...  ...         0\n",
              "1.326200e+18  nhs to be ready to roll out coronavirus vaccin...  ...         2\n",
              "1.329030e+18  so its gone up just as a new vaccine announced...  ...         1\n",
              "1.332280e+18  south korea says it foiled north korea attempt...  ...         2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "LbED8Q185xWu",
        "outputId": "346e4f95-bb19-4af8-917a-f7c89b3828f3"
      },
      "source": [
        "display(our_data.shape) #Number of rows (instances) and columns in the dataset\n",
        "print(our_data[\"label\"].value_counts()/our_data.shape[0]) #Class distribution in the dataset"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2792, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Neutral    0.361748\n",
            "ProVax     0.354943\n",
            "AntiVax    0.283309\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYW_S3585xXF"
      },
      "source": [
        "# Prepare Dataset\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['tweet'] = our_data.tweet\n",
        "trainDF['label'] = our_data.label\n",
        "\n",
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['tweet'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "# train_y = encoder.fit_transform(trainDF['label'])\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "NQzeWmeMsatO",
        "outputId": "c5880085-261b-40f7-f199-87d8b2e55b3b"
      },
      "source": [
        "trainDF"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.330830e+18</th>\n",
              "      <td>a quick reminder that cambridge not known for ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325910e+18</th>\n",
              "      <td>really good news pfizer and biontech report th...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326200e+18</th>\n",
              "      <td>nhs to be ready to roll out coronavirus vaccin...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329030e+18</th>\n",
              "      <td>so its gone up just as a new vaccine announced...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.332280e+18</th>\n",
              "      <td>south korea says it foiled north korea attempt...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.334160e+18</th>\n",
              "      <td>todays coronavirus news ontario is reporting c...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325770e+18</th>\n",
              "      <td>i love that trump said uhh uhh coronavirus is ...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.336230e+18</th>\n",
              "      <td>covid vaccine first person receives #pfizer #c...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.333440e+18</th>\n",
              "      <td>moderna to ask health regulators to authorize ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325810e+18</th>\n",
              "      <td>jdamis i keep seeing this touted as exceptiona...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2792 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet    label\n",
              "id                                                                      \n",
              "1.330830e+18  a quick reminder that cambridge not known for ...  Neutral\n",
              "1.325910e+18  really good news pfizer and biontech report th...   ProVax\n",
              "1.326200e+18  nhs to be ready to roll out coronavirus vaccin...  Neutral\n",
              "1.329030e+18  so its gone up just as a new vaccine announced...  AntiVax\n",
              "1.332280e+18  south korea says it foiled north korea attempt...  Neutral\n",
              "...                                                         ...      ...\n",
              "1.334160e+18  todays coronavirus news ontario is reporting c...  Neutral\n",
              "1.325770e+18  i love that trump said uhh uhh coronavirus is ...   ProVax\n",
              "1.336230e+18  covid vaccine first person receives #pfizer #c...  Neutral\n",
              "1.333440e+18  moderna to ask health regulators to authorize ...  Neutral\n",
              "1.325810e+18  jdamis i keep seeing this touted as exceptiona...  AntiVax\n",
              "\n",
              "[2792 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOKz8xQr5xXJ"
      },
      "source": [
        "### Section 2: Text Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhC5TZuL5xXK"
      },
      "source": [
        "Typical steps involve tokenization, lower casing, removing, stop words, punctuation markers etc, and vectorization. Other processes such as stemming/lemmatization can also be performed. Here, we are performing the following steps: removing br tags, punctuation, numbers, and stopwords. While we are using sklearn's list of stopwords, there are several other stop word lists (e.g., from NLTK) or sometimes, custom stopword lists are needed depending on the task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZSHdHZ5xXL"
      },
      "source": [
        "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
        "def clean(doc): #doc is a string of text\n",
        "    # doc = doc.replace(\"@\", \" \")\n",
        "    # doc = doc.replace(\" \", \" \")\n",
        "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
        "    # doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
        "    #remove punctuation and numbers\n",
        "    return doc"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpVmfv8KKswH"
      },
      "source": [
        "import re\n",
        "def remove_emojis(data):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bI2QiCPIT0"
      },
      "source": [
        "def remove_url(data):\n",
        "  url = re.compile(\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\", re.UNICODE)\n",
        "  return re.sub(url, '', data)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi371W3mGgmm"
      },
      "source": [
        "# for i in range(len(our_data)):\n",
        "#   our_data.tweet.iloc[i] = remove_url(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = clean(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = remove_emojis(our_data.tweet.iloc[i])"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-Fd5XT2hHDCx",
        "outputId": "06c0ceec-4569-4283-f808-af3e674b01fc"
      },
      "source": [
        "our_data.head()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>1.326540e+18</td>\n",
              "      <td>no company or user will directly profit from #...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>1.334250e+18</td>\n",
              "      <td>cdc vaccine advisers vote on which groups shou...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>1.329030e+18</td>\n",
              "      <td>now what the fuck is this efficacy revision of...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>1.325770e+18</td>\n",
              "      <td>some good news the covid vaccine being develop...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1442</th>\n",
              "      <td>1.330800e+18</td>\n",
              "      <td>norbertelekes hope some vaccine results will c...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                id  ... label_num\n",
              "827   1.326540e+18  ...         2\n",
              "330   1.334250e+18  ...         2\n",
              "487   1.329030e+18  ...         1\n",
              "348   1.325770e+18  ...         0\n",
              "1442  1.330800e+18  ...         0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CfVm42o5xXS"
      },
      "source": [
        "# Section 3: Modeling\n",
        "\n",
        "Now we are ready for the modelling. We are going to use algorithms from sklearn package. We will go through the following steps:\n",
        "\n",
        "1 Read train data    \n",
        "2 Extract features from the training data using CountVectorizer, which is a bag of words feature  implementation. We will use the pre-processing function above in conjunction with Count Vectorizer  \n",
        "3 Transform the test data into the same feature vector as the training data.  \n",
        "4 Train the classifier  \n",
        "5 Evaluate the classifier  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BscPlX2tZOPS"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKL1ucwxswDu",
        "outputId": "44ab4e28-da3b-436d-a82c-d1556a8b7250"
      },
      "source": [
        "X_train = train_x\n",
        "# X_train = trainDF['tweet']\n",
        "y_train =train_y\n",
        "print(X_train.shape, y_train.shape, valid_x.shape, valid_y.shape)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094,) (2094,) (698,) (698,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHC3O8EwuJ1Q"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBoBD2BsZVUW"
      },
      "source": [
        " ### Count Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsUyIBUD5xZI"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['tweet'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "X_train_dtm =  count_vect.transform(X_train)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjNXiaiP91UA",
        "outputId": "92dd2978-c5a1-4a27-ca48-1169415f57d2"
      },
      "source": [
        "print(X_train_dtm.shape, xvalid_count.shape)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 7716) (698, 7716)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYP5Sg-VaqEi"
      },
      "source": [
        "### TF-IDF Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KgnBDQAamup",
        "outputId": "43178e87-2b73-4aff-f6da-9ba8c106d106"
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "print(xtrain_tfidf.shape, xvalid_tfidf.shape)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "print(xtrain_tfidf_ngram.shape, xvalid_tfidf_ngram.shape)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train)\n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
        "print(xtrain_tfidf_ngram_chars.shape, xvalid_tfidf_ngram_chars.shape)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT4c02k7uTIl",
        "outputId": "1f707414-8c4f-4a65-a1ee-d981770abce8"
      },
      "source": [
        "print(xtrain_tfidf_ngram)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4770)\t0.286356967336268\n",
            "  (0, 4537)\t0.23613552329447768\n",
            "  (0, 4518)\t0.30261147337599364\n",
            "  (0, 4517)\t0.21924968854523927\n",
            "  (0, 4282)\t0.2658787115734172\n",
            "  (0, 3782)\t0.286356967336268\n",
            "  (0, 3464)\t0.24703695015549074\n",
            "  (0, 3261)\t0.30261147337599364\n",
            "  (0, 3238)\t0.19681550611370044\n",
            "  (0, 3195)\t0.29366597318366866\n",
            "  (0, 876)\t0.2748242117657422\n",
            "  (0, 864)\t0.13144551705544316\n",
            "  (0, 195)\t0.30261147337599364\n",
            "  (0, 194)\t0.30261147337599364\n",
            "  (0, 118)\t0.12795735759481333\n",
            "  (1, 4804)\t0.16756732145158726\n",
            "  (1, 4441)\t0.2751674834341599\n",
            "  (1, 4438)\t0.21722730462171483\n",
            "  (1, 4210)\t0.2821908876982268\n",
            "  (1, 4033)\t0.148205016102205\n",
            "  (1, 2943)\t0.2425278136648661\n",
            "  (1, 2942)\t0.2425278136648661\n",
            "  (1, 2650)\t0.26922927974039096\n",
            "  (1, 2175)\t0.23738390765157036\n",
            "  (1, 2174)\t0.20949705350404296\n",
            "  :\t:\n",
            "  (2092, 1931)\t0.21285747806848465\n",
            "  (2092, 1842)\t0.17257547764014033\n",
            "  (2092, 1841)\t0.17257547764014033\n",
            "  (2092, 959)\t0.12027060638112788\n",
            "  (2092, 921)\t0.06033190563492359\n",
            "  (2092, 566)\t0.1874187018300794\n",
            "  (2092, 304)\t0.21934143189655905\n",
            "  (2092, 302)\t0.16385524212546443\n",
            "  (2092, 195)\t0.21934143189655905\n",
            "  (2092, 194)\t0.21934143189655905\n",
            "  (2093, 4806)\t0.24592300507731948\n",
            "  (2093, 4804)\t0.15877078255201707\n",
            "  (2093, 4552)\t0.26737712151017645\n",
            "  (2093, 4551)\t0.2607224147241206\n",
            "  (2093, 4063)\t0.2607224147241206\n",
            "  (2093, 3470)\t0.25022206647437956\n",
            "  (2093, 2720)\t0.22727795396683312\n",
            "  (2093, 1257)\t0.18407054044855925\n",
            "  (2093, 1009)\t0.2607224147241206\n",
            "  (2093, 697)\t0.27552182437092176\n",
            "  (2093, 696)\t0.27552182437092176\n",
            "  (2093, 544)\t0.21329879985629727\n",
            "  (2093, 522)\t0.2607224147241206\n",
            "  (2093, 206)\t0.27552182437092176\n",
            "  (2093, 203)\t0.38670008923056154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs7PJfyRb5dH"
      },
      "source": [
        "### Word Embeddings  --> Not Working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEZqudNOb7S9"
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(our_data['tweet'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAWfjkCl7oH"
      },
      "source": [
        "### Text / NLP based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN761pknl-tU"
      },
      "source": [
        "trainDF = our_data\n",
        "trainDF['char_count'] = trainDF['tweet'].apply(len)\n",
        "trainDF['word_count'] = trainDF['tweet'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rtfX1omKsr"
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "trainDF['noun_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR62UJuynCDW"
      },
      "source": [
        "### Topic Models as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73aiWP0InCgB"
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(X_train_dtm)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCmJpXB0oiYh",
        "outputId": "204ceb83-bfc8-4a60-f3e3-37a608537235"
      },
      "source": [
        "topic_summaries"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vaccinations chances six miss modernavaccine revlox conduct renders extended mutate',\n",
              " 'demand difficult substantial thoughts organized liquid gold duck ddale maintain',\n",
              " 'preparing similar ur rehman atta dr squashtheclot suits unveil shipping',\n",
              " 'wow leading congratulations quite sputnikvaccine underlying yale onboard value replication',\n",
              " 'travel canadians explains warn finance dailybriefing debunked stockmarket retrigger roars',\n",
              " 'seems gonna trump completely america wrong rollout freedom transition youre',\n",
              " 'china begin mid shut spray nasal wantai stage agency expert',\n",
              " 'credit send patent rogue tn referendum fyi profits generic gives',\n",
              " 'funder qqq dia spy err wonderwoman meaning programmes prepare qanon',\n",
              " 'india times doses serum writes institute buy trained sections table',\n",
              " 'evil bye jail coalition positions ariana tour swab deliveries abs',\n",
              " 'petition sign dna lockdowns via scientific billgates pm meeting meet',\n",
              " 'the vaccine to of covid and be will coronavirus in',\n",
              " 'russia half knowledge significant lord sputnik majority transport talks doses',\n",
              " 'listen answers mabsp apple concorda da afirmao analyses basis editora',\n",
              " 'speed warp operation of are trumps fools promised covid covidvaccine',\n",
              " 'free overnight nationwide card suspicious bcpoli redwolvezden crof despise bblock',\n",
              " 'vaccine the covid to is and of in for news',\n",
              " 'vaccine covid coronavirus of says trials in astrazeneca oxford candidate',\n",
              " 'the vaccine it that not to you and take my']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXKcs3N3aHwT"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qY2Ug9KyTqN"
      },
      "source": [
        "### Non Deep Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMrq29lY8Rj",
        "outputId": "7f021972-dbf4-4493-8ef8-76cf78c58d2b"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "nb = MultinomialNB() #instantiate a Multinomial Naive Bayes model\n",
        "nb.fit(X_train_dtm, y_train)#train the count vectorizer model\n",
        "# nb.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# nb.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# nb.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v7pM9hB5xbA",
        "outputId": "b093bfd5-ef10-4153-ab12-9d28acded3ac"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "logreg = LogisticRegression(class_weight=\"balanced\",max_iter=1000) #instantiate a logistic regression model\n",
        "# logreg.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "logreg.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# logreg.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# logreg.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLKusAQ5xbf",
        "outputId": "44bdcf6a-5651-49d9-dc45-67b4ba80ffa9"
      },
      "source": [
        "# Linear SVM Classifier\n",
        "classifier = LinearSVC(class_weight='balanced') #instantiate a logistic regression model\n",
        "# classifier.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNPTNoq1puGY",
        "outputId": "efb02b5e-e289-4e4c-9eb7-f1a1802bed55"
      },
      "source": [
        "# Bagging Model\n",
        "classifier = ensemble.RandomForestClassifier()\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "# classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiszTNxCqBlI",
        "outputId": "f3b850ce-aea4-46fc-ba0d-fc25480c7040"
      },
      "source": [
        "# Extereme Gradient Boosting\n",
        "EXB = xgboost.XGBClassifier()\n",
        "EXB.fit(X_train_dtm.tocsc(), y_train)\n",
        "# EXB.fit(xtrain_tfidf.tocsc(), y_train)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59v5MNTDqh7b",
        "outputId": "3c29c74e-c197-4043-9ed5-b5ac013980dc"
      },
      "source": [
        "# Shallow Neural Network\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
        "classifier.fit(xtrain_tfidf.toarray(), y_train,epochs =1, shuffle = True)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 1s 4ms/step - loss: 0.5355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e12d53450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQFa-Q54yNxa"
      },
      "source": [
        "### Deep Neural Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpb96BDRwid6"
      },
      "source": [
        "# CNN\n",
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "    \n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "classifier.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later\n",
        "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "# print \"CNN, Word Embeddings\",  accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DxmpbSQxt4_"
      },
      "source": [
        "#  LSTM\n",
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "classifier.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUjJbbzzycW_"
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "classifier.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWrlNY6O3GlI"
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "classifier.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zNGS61S3dvn"
      },
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "classifier.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcMKcvRgaLOK"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKaOn7EUDAYi"
      },
      "source": [
        "### Naive Bayes Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6KeYb_t4pKw",
        "outputId": "07e079a1-0feb-490d-df9e-2265d1a04064"
      },
      "source": [
        "predictions = nb.predict(xvalid_count)\n",
        "print(\" NB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, Count Vectors: 0.7320916905444126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhxvc2rCBEw",
        "outputId": "83bbb7f4-51a8-4de2-97c0-781cfb378395"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf)\n",
        "print(\" NB, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, WordLevel TF-IDF: 0.7134670487106017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxBrixtvCI7J",
        "outputId": "86ed6ec9-a422-4f23-8151-9c3fddb4f5e1"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram)\n",
        "print(\" NB, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, N-Gram Vectors: 0.6762177650429799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVHjLFMgCJo_",
        "outputId": "e664b356-509f-4319-9ac8-3083c9ffa701"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" NB, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, CharLevel Vectors: 0.673352435530086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPOVg0pDMTK"
      },
      "source": [
        "### Linear Classification --> Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2RGbV5EpX5",
        "outputId": "45d38b65-9527-488f-9727-5a93741bd826"
      },
      "source": [
        "predictions = logreg.predict(xvalid_count)\n",
        "print(\" LR, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, Count Vectors: 0.7134670487106017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdyB1-njEpX9",
        "outputId": "694cd428-5fe4-4ef7-d183-075fc8c5d1ba"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf)\n",
        "print(\" LR, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, WordLevel TF-IDF: 0.7306590257879656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0BO3ZGaEpX-",
        "outputId": "f4a5e660-2ea7-4fe3-d2db-2719e339fda1"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram)\n",
        "print(\" LR, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, N-Gram Vectors: 0.6776504297994269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNyApNB0EpYA",
        "outputId": "4e58f361-038f-45c7-ab6d-5326a0488e34"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" LR, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, CharLevel Vectors: 0.7048710601719198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odFGh1iEFgP4"
      },
      "source": [
        "### SVM Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCeidP4_FgQM",
        "outputId": "d3887a84-b92d-40cd-bb54-b7d141b12ae5"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" SVM, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, Count Vectors: 0.7020057306590258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFePpn7jFgQN",
        "outputId": "f336972b-5bd5-4dab-ce8c-f3a8b2212754"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" SVM, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, WordLevel TF-IDF: 0.7392550143266475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y33Gg9uJFgQN",
        "outputId": "78e3f14d-e732-4867-a125-53ffda1ebbd9"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" SVM, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, N-Gram Vectors: 0.663323782234957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoPXz5yHFgQO",
        "outputId": "2ba457c2-fad4-4e0c-a00d-c3d381307545"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" SVM, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, CharLevel Vectors: 0.7177650429799427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7TuyLjUGp3H"
      },
      "source": [
        "### Random Forrest Evaluation Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdCzAPlxGp3J",
        "outputId": "c14741f4-b5d2-4311-91a7-fc4b84b18b69"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" RF, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, Count Vectors: 0.6934097421203438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bb_LaiEGp3L",
        "outputId": "8f62ac1b-0a74-49be-bd27-b2451b824696"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.6905444126074498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKaPM_l4Gp3N",
        "outputId": "c74505c3-e122-4038-d770-b2f8f208213e"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" RF, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, N-Gram Vectors: 0.6532951289398281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz19N6xAGp3O",
        "outputId": "08f14f01-66b4-4ba4-b38f-2960135dc45f"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" RF, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, CharLevel Vectors: 0.6432664756446992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8GM_FdKdyu"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YlSs-eiKcxi",
        "outputId": "9811087d-8ec8-4e84-9b63-8dbf71676081"
      },
      "source": [
        "predictions = EXB.predict(xvalid_count.tocsc())\n",
        "print(\" EXB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " EXB, Count Vectors: 0.6977077363896849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPsEyHWLAUD",
        "outputId": "ccf787ce-4a67-4ada-d6f5-b5f696cde024"
      },
      "source": [
        "predictions = EXB.predict(xvalid_tfidf.tocsc())\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.6805157593123209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5QeBxTLccA"
      },
      "source": [
        "### Shallow NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-8D2HxLgdb",
        "outputId": "c62bb1bc-4c4f-48b3-ad8d-f05b67be9480"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" Shallow NN, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Shallow NN, WordLevel TF-IDF: 0.27793696275071633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eInI6X92DFhR"
      },
      "source": [
        "### Deep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMZt_DwiNNAa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}