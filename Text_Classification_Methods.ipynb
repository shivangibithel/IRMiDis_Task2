{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "janWv1vG5xUD",
        "bQGOVFFwaepj",
        "fOKz8xQr5xXJ",
        "fBoBD2BsZVUW",
        "qYP5Sg-VaqEi",
        "bpAWfjkCl7oH",
        "sR62UJuynCDW",
        "8qY2Ug9KyTqN",
        "lKaOn7EUDAYi",
        "gWPOVg0pDMTK",
        "odFGh1iEFgP4",
        "w7TuyLjUGp3H",
        "Ka8GM_FdKdyu",
        "oU5QeBxTLccA",
        "eInI6X92DFhR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivangibithel/IRMiDis_Task2/blob/main/Text_Classification_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27hY5InNb60"
      },
      "source": [
        "[A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janWv1vG5xUD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PV3HrLnLJAY"
      },
      "source": [
        "# pip install emoji --upgrade"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvvarqE5xWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644b981d-b50d-437c-aafb-2b140de10a43"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd #to work with csv files\n",
        "\n",
        "#matplotlib imports are used to plot confusion matrices for the classifiers\n",
        "import matplotlib as mpl \n",
        "import matplotlib.cm as cm \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#import feature extraction methods from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "\n",
        "#pre-processing of text\n",
        "import string\n",
        "import re\n",
        "\n",
        "#import classifiers from sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#import different metrics to evaluate the classifiers\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn import metrics\n",
        "\n",
        "#import time function from time module to track the training duration\n",
        "from time import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFWAClI3mkMK",
        "outputId": "550ba0a9-91a0-44b4-84e5-7c3ba3977e71"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGOVFFwaepj"
      },
      "source": [
        "# Section 1: Load and explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iPh67KcCh5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1683dea1-5b6f-4c6a-c672-58af2d077789"
      },
      "source": [
        "our_data = pd.read_csv(\"cleaned_train.csv\",index_col = \"id\")\n",
        "test_data = pd.read_csv(\"cleaned_test.csv\",index_col = \"id\")\n",
        "our_data = shuffle(our_data)\n",
        "our_data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.326010e+18</th>\n",
              "      <td>fyi realdonaldtrumpthat pfizer #coronavirus va...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325730e+18</th>\n",
              "      <td>dr carrie madej warns about coronavirus vaccin...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326220e+18</th>\n",
              "      <td>absolutely not everyone has bodily autonomy va...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326230e+18</th>\n",
              "      <td>covid vaccine is hereeeeeeee</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325770e+18</th>\n",
              "      <td>fantastic news pfizer says early analysis show...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet  ... label_num\n",
              "id                                                               ...          \n",
              "1.326010e+18  fyi realdonaldtrumpthat pfizer #coronavirus va...  ...         2\n",
              "1.325730e+18  dr carrie madej warns about coronavirus vaccin...  ...         1\n",
              "1.326220e+18  absolutely not everyone has bodily autonomy va...  ...         1\n",
              "1.326230e+18                       covid vaccine is hereeeeeeee  ...         0\n",
              "1.325770e+18  fantastic news pfizer says early analysis show...  ...         0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LbED8Q185xWu",
        "outputId": "288d0039-9b75-4f5a-9221-ffb6d0aff1c4"
      },
      "source": [
        "display(our_data.shape) #Number of rows (instances) and columns in the dataset\n",
        "print(our_data[\"label\"].value_counts()/our_data.shape[0]) #Class distribution in the dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2792, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Neutral    0.361748\n",
            "ProVax     0.354943\n",
            "AntiVax    0.283309\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYW_S3585xXF"
      },
      "source": [
        "# Prepare Dataset\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['tweet'] = our_data.tweet\n",
        "trainDF['label'] = our_data.label\n",
        "\n",
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['tweet'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "# train_y = encoder.fit_transform(trainDF['label'])\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NQzeWmeMsatO",
        "outputId": "6f8ce3ec-4f50-4e19-eb8c-060c13488f2b"
      },
      "source": [
        "trainDF"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.326010e+18</th>\n",
              "      <td>fyi realdonaldtrumpthat pfizer #coronavirus va...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325730e+18</th>\n",
              "      <td>dr carrie madej warns about coronavirus vaccin...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326220e+18</th>\n",
              "      <td>absolutely not everyone has bodily autonomy va...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326230e+18</th>\n",
              "      <td>covid vaccine is hereeeeeeee</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325770e+18</th>\n",
              "      <td>fantastic news pfizer says early analysis show...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325770e+18</th>\n",
              "      <td>such a groundbreaking day finally after nearly...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.333420e+18</th>\n",
              "      <td>vespasian mongibellus davekeating very nice bu...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325860e+18</th>\n",
              "      <td>will fordnation botch the covid vaccine like t...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325900e+18</th>\n",
              "      <td>do mrna covid vaccines permanently alter human...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325830e+18</th>\n",
              "      <td>novavax on track to begin us trial of covid va...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2792 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet    label\n",
              "id                                                                      \n",
              "1.326010e+18  fyi realdonaldtrumpthat pfizer #coronavirus va...  Neutral\n",
              "1.325730e+18  dr carrie madej warns about coronavirus vaccin...  AntiVax\n",
              "1.326220e+18  absolutely not everyone has bodily autonomy va...  AntiVax\n",
              "1.326230e+18                       covid vaccine is hereeeeeeee   ProVax\n",
              "1.325770e+18  fantastic news pfizer says early analysis show...   ProVax\n",
              "...                                                         ...      ...\n",
              "1.325770e+18  such a groundbreaking day finally after nearly...   ProVax\n",
              "1.333420e+18  vespasian mongibellus davekeating very nice bu...  Neutral\n",
              "1.325860e+18  will fordnation botch the covid vaccine like t...  AntiVax\n",
              "1.325900e+18  do mrna covid vaccines permanently alter human...  Neutral\n",
              "1.325830e+18  novavax on track to begin us trial of covid va...  Neutral\n",
              "\n",
              "[2792 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOKz8xQr5xXJ"
      },
      "source": [
        "### Section 2: Text Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhC5TZuL5xXK"
      },
      "source": [
        "Typical steps involve tokenization, lower casing, removing, stop words, punctuation markers etc, and vectorization. Other processes such as stemming/lemmatization can also be performed. Here, we are performing the following steps: removing br tags, punctuation, numbers, and stopwords. While we are using sklearn's list of stopwords, there are several other stop word lists (e.g., from NLTK) or sometimes, custom stopword lists are needed depending on the task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZSHdHZ5xXL"
      },
      "source": [
        "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
        "def clean(doc): #doc is a string of text\n",
        "    # doc = doc.replace(\"@\", \" \")\n",
        "    # doc = doc.replace(\" \", \" \")\n",
        "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
        "    # doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
        "    #remove punctuation and numbers\n",
        "    return doc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpVmfv8KKswH"
      },
      "source": [
        "import re\n",
        "def remove_emojis(data):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bI2QiCPIT0"
      },
      "source": [
        "def remove_url(data):\n",
        "  url = re.compile(\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\", re.UNICODE)\n",
        "  return re.sub(url, '', data)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi371W3mGgmm"
      },
      "source": [
        "# for i in range(len(our_data)):\n",
        "#   our_data.tweet.iloc[i] = remove_url(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = clean(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = remove_emojis(our_data.tweet.iloc[i])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-Fd5XT2hHDCx",
        "outputId": "0127dd32-44b8-438e-e8b2-28a463bc6b2d"
      },
      "source": [
        "our_data.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.326010e+18</th>\n",
              "      <td>fyi realdonaldtrumpthat pfizer #coronavirus va...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325730e+18</th>\n",
              "      <td>dr carrie madej warns about coronavirus vaccin...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326220e+18</th>\n",
              "      <td>absolutely not everyone has bodily autonomy va...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326230e+18</th>\n",
              "      <td>covid vaccine is hereeeeeeee</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325770e+18</th>\n",
              "      <td>fantastic news pfizer says early analysis show...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet  ... label_num\n",
              "id                                                               ...          \n",
              "1.326010e+18  fyi realdonaldtrumpthat pfizer #coronavirus va...  ...         2\n",
              "1.325730e+18  dr carrie madej warns about coronavirus vaccin...  ...         1\n",
              "1.326220e+18  absolutely not everyone has bodily autonomy va...  ...         1\n",
              "1.326230e+18                       covid vaccine is hereeeeeeee  ...         0\n",
              "1.325770e+18  fantastic news pfizer says early analysis show...  ...         0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CfVm42o5xXS"
      },
      "source": [
        "# Section 3: Modeling\n",
        "\n",
        "Now we are ready for the modelling. We are going to use algorithms from sklearn package. We will go through the following steps:\n",
        "\n",
        "1 Read train data    \n",
        "2 Extract features from the training data using CountVectorizer, which is a bag of words feature  implementation. We will use the pre-processing function above in conjunction with Count Vectorizer  \n",
        "3 Transform the test data into the same feature vector as the training data.  \n",
        "4 Train the classifier  \n",
        "5 Evaluate the classifier  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BscPlX2tZOPS"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKL1ucwxswDu",
        "outputId": "03a5688f-d19b-415c-b638-68cd6197177f"
      },
      "source": [
        "X_train = train_x\n",
        "# X_train = trainDF['tweet']\n",
        "y_train =train_y\n",
        "print(X_train.shape, y_train.shape, valid_x.shape, valid_y.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094,) (2094,) (698,) (698,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHC3O8EwuJ1Q",
        "outputId": "a3365d0f-f655-459a-fba6-d901bf525512"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "1.326600e+18    fighting stigma govt has a plan to ensure covi...\n",
              "1.335960e+18    mrna long the canadian government has doubled ...\n",
              "1.329440e+18    astrazeneca vaccine shows strong immune respon...\n",
              "1.325790e+18    do they think we are fucking stupid just days ...\n",
              "1.325800e+18    donaldjtrumpjr all pharmaceutical companies ar...\n",
              "                                      ...                        \n",
              "1.330910e+18    what are the potential #supplychain challenges...\n",
              "1.325920e+18    tomfole spduckworth camestewart i think a mand...\n",
              "1.326080e+18    me i refuse to take the mark of the beast covi...\n",
              "1.328060e+18    govmikedewine and we need rapid home frequent ...\n",
              "1.325840e+18    never related to a tweet so much in my life fu...\n",
              "Name: tweet, Length: 2094, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBoBD2BsZVUW"
      },
      "source": [
        " ### Count Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsUyIBUD5xZI"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['tweet'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "X_train_dtm =  count_vect.transform(X_train)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjNXiaiP91UA",
        "outputId": "c749ef36-423a-455e-e641-ebbd55b158aa"
      },
      "source": [
        "print(X_train_dtm.shape, xvalid_count.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 7716) (698, 7716)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYP5Sg-VaqEi"
      },
      "source": [
        "### TF-IDF Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KgnBDQAamup",
        "outputId": "fc804e22-b2ed-4cdb-df36-8e32ddbf73a6"
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "print(xtrain_tfidf.shape, xvalid_tfidf.shape)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "print(xtrain_tfidf_ngram.shape, xvalid_tfidf_ngram.shape)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train)\n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
        "print(xtrain_tfidf_ngram_chars.shape, xvalid_tfidf_ngram_chars.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT4c02k7uTIl",
        "outputId": "fb2e354a-320d-4d4f-c085-9b0ca4290bdb"
      },
      "source": [
        "print(xtrain_tfidf_ngram)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4155)\t0.20712033129118507\n",
            "  (0, 4066)\t0.3042002761324843\n",
            "  (0, 2962)\t0.3233232635365891\n",
            "  (0, 1666)\t0.28507728872837956\n",
            "  (0, 995)\t0.20235232602790756\n",
            "  (0, 915)\t0.3560142264282778\n",
            "  (0, 180)\t0.336891239024173\n",
            "  (0, 178)\t0.2609851738370408\n",
            "  (0, 155)\t0.27561303864875997\n",
            "  (0, 75)\t0.3560142264282778\n",
            "  (0, 74)\t0.3560142264282778\n",
            "  (1, 4397)\t0.22733882991202398\n",
            "  (1, 4099)\t0.28984530578623174\n",
            "  (1, 4098)\t0.28984530578623174\n",
            "  (1, 4033)\t0.1561104810608021\n",
            "  (1, 3135)\t0.22453931344602923\n",
            "  (1, 3134)\t0.22192054068412514\n",
            "  (1, 3104)\t0.3062978206920625\n",
            "  (1, 2744)\t0.2972433483571516\n",
            "  (1, 2380)\t0.28984530578623174\n",
            "  (1, 2372)\t0.3062978206920625\n",
            "  (1, 2368)\t0.2430731509456039\n",
            "  (1, 1631)\t0.2972433483571516\n",
            "  (1, 1283)\t0.2835903499173156\n",
            "  (1, 952)\t0.2475863418046144\n",
            "  :\t:\n",
            "  (2091, 1699)\t0.23633778910428332\n",
            "  (2091, 921)\t0.05068763119162931\n",
            "  (2091, 729)\t0.17883143638795057\n",
            "  (2092, 4690)\t0.26161637661656845\n",
            "  (2092, 4318)\t0.2167005761515349\n",
            "  (2092, 4170)\t0.30690369765155834\n",
            "  (2092, 4155)\t0.19660198538385398\n",
            "  (2092, 4074)\t0.20006642935431587\n",
            "  (2092, 3821)\t0.30690369765155834\n",
            "  (2092, 3819)\t0.14479878099391114\n",
            "  (2092, 3732)\t0.2818508338671341\n",
            "  (2092, 2704)\t0.2758729042545538\n",
            "  (2092, 2690)\t0.16852399642555482\n",
            "  (2092, 2624)\t0.3379344910485628\n",
            "  (2092, 2623)\t0.3016307900388162\n",
            "  (2092, 848)\t0.26588322005341275\n",
            "  (2092, 457)\t0.24626343568675466\n",
            "  (2092, 358)\t0.2758729042545538\n",
            "  (2093, 4020)\t0.32605159198606726\n",
            "  (2093, 3819)\t0.19603788240040085\n",
            "  (2093, 3267)\t0.4235992266636036\n",
            "  (2093, 3105)\t0.4235992266636036\n",
            "  (2093, 2433)\t0.4083671211876842\n",
            "  (2093, 1909)\t0.3962045198594826\n",
            "  (2093, 1507)\t0.41550592191100233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs7PJfyRb5dH"
      },
      "source": [
        "### Word Embeddings  --> Not Working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW11l6hplDuI",
        "outputId": "fa40ec21-709f-4cb2-9e25-fbdcacec613f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37yUHD1UlTqz"
      },
      "source": [
        "%cd gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucorOg3FkQko"
      },
      "source": [
        "Can also use pre-train embeddings from Gensim library in future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEZqudNOb7S9"
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(our_data['tweet'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAWfjkCl7oH"
      },
      "source": [
        "### Text / NLP based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN761pknl-tU"
      },
      "source": [
        "trainDF = our_data\n",
        "trainDF['char_count'] = trainDF['tweet'].apply(len)\n",
        "trainDF['word_count'] = trainDF['tweet'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rtfX1omKsr"
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "trainDF['noun_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR62UJuynCDW"
      },
      "source": [
        "### Topic Models as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73aiWP0InCgB"
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(X_train_dtm)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCmJpXB0oiYh"
      },
      "source": [
        "topic_summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXKcs3N3aHwT"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qY2Ug9KyTqN"
      },
      "source": [
        "### Non Deep Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMrq29lY8Rj",
        "outputId": "7f021972-dbf4-4493-8ef8-76cf78c58d2b"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "nb = MultinomialNB() #instantiate a Multinomial Naive Bayes model\n",
        "nb.fit(X_train_dtm, y_train)#train the count vectorizer model\n",
        "# nb.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# nb.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# nb.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v7pM9hB5xbA",
        "outputId": "b093bfd5-ef10-4153-ab12-9d28acded3ac"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "logreg = LogisticRegression(class_weight=\"balanced\",max_iter=1000) #instantiate a logistic regression model\n",
        "# logreg.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "logreg.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# logreg.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# logreg.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLKusAQ5xbf",
        "outputId": "44bdcf6a-5651-49d9-dc45-67b4ba80ffa9"
      },
      "source": [
        "# Linear SVM Classifier\n",
        "classifier = LinearSVC(class_weight='balanced') #instantiate a logistic regression model\n",
        "# classifier.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNPTNoq1puGY",
        "outputId": "efb02b5e-e289-4e4c-9eb7-f1a1802bed55"
      },
      "source": [
        "# Bagging Model\n",
        "classifier = ensemble.RandomForestClassifier()\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "# classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiszTNxCqBlI",
        "outputId": "f3b850ce-aea4-46fc-ba0d-fc25480c7040"
      },
      "source": [
        "# Extereme Gradient Boosting\n",
        "EXB = xgboost.XGBClassifier()\n",
        "EXB.fit(X_train_dtm.tocsc(), y_train)\n",
        "# EXB.fit(xtrain_tfidf.tocsc(), y_train)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59v5MNTDqh7b",
        "outputId": "3c29c74e-c197-4043-9ed5-b5ac013980dc"
      },
      "source": [
        "# Shallow Neural Network\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
        "classifier.fit(xtrain_tfidf.toarray(), y_train,epochs =1, shuffle = True)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 1s 4ms/step - loss: 0.5355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e12d53450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQFa-Q54yNxa"
      },
      "source": [
        "### Deep Neural Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpb96BDRwid6",
        "outputId": "43e03bc1-4e98-4a9b-f20c-aae2986232b4"
      },
      "source": [
        "# CNN\n",
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "    \n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "classifier.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later\n",
        "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "# print \"CNN, Word Embeddings\",  accuracy"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 3s 26ms/step - loss: 0.0742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe206a995d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvWJpN84oRL5",
        "outputId": "79b68281-4ab5-4045-bcda-aa07dc026af1"
      },
      "source": [
        "train_seq_x.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2094, 70)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DxmpbSQxt4_",
        "outputId": "17451fcf-cc8c-4c9d-faa9-71d3f1329e23"
      },
      "source": [
        "#  LSTM\n",
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_lstm = create_rnn_lstm()\n",
        "classifier_lstm.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 8s 88ms/step - loss: -0.0712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe206cf68d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUjJbbzzycW_",
        "outputId": "ac8c7ac2-ff4e-4664-cf3c-7c21ec3e0b4d"
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_rnn = create_rnn_gru()\n",
        "classifier_rnn.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 7s 74ms/step - loss: -0.0862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe203a1b1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWrlNY6O3GlI",
        "outputId": "27a52a16-786e-4150-d51e-226820c55dfd"
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_brnn = create_bidirectional_rnn()\n",
        "classifier_brnn.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 12s 129ms/step - loss: -0.3691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe202c274d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zNGS61S3dvn",
        "outputId": "62013a98-201d-46dd-f328-0dbe52249baa"
      },
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_rcnn = create_rcnn()\n",
        "classifier_rcnn.fit(train_seq_x, train_y)\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 3s 27ms/step - loss: -0.1242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2018f8950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcMKcvRgaLOK"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKaOn7EUDAYi"
      },
      "source": [
        "### Naive Bayes Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6KeYb_t4pKw",
        "outputId": "07e079a1-0feb-490d-df9e-2265d1a04064"
      },
      "source": [
        "predictions = nb.predict(xvalid_count)\n",
        "print(\" NB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, Count Vectors: 0.7320916905444126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhxvc2rCBEw",
        "outputId": "83bbb7f4-51a8-4de2-97c0-781cfb378395"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf)\n",
        "print(\" NB, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, WordLevel TF-IDF: 0.7134670487106017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxBrixtvCI7J",
        "outputId": "86ed6ec9-a422-4f23-8151-9c3fddb4f5e1"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram)\n",
        "print(\" NB, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, N-Gram Vectors: 0.6762177650429799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVHjLFMgCJo_",
        "outputId": "e664b356-509f-4319-9ac8-3083c9ffa701"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" NB, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, CharLevel Vectors: 0.673352435530086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPOVg0pDMTK"
      },
      "source": [
        "### Linear Classification --> Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2RGbV5EpX5",
        "outputId": "45d38b65-9527-488f-9727-5a93741bd826"
      },
      "source": [
        "predictions = logreg.predict(xvalid_count)\n",
        "print(\" LR, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, Count Vectors: 0.7134670487106017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdyB1-njEpX9",
        "outputId": "694cd428-5fe4-4ef7-d183-075fc8c5d1ba"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf)\n",
        "print(\" LR, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, WordLevel TF-IDF: 0.7306590257879656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0BO3ZGaEpX-",
        "outputId": "f4a5e660-2ea7-4fe3-d2db-2719e339fda1"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram)\n",
        "print(\" LR, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, N-Gram Vectors: 0.6776504297994269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNyApNB0EpYA",
        "outputId": "4e58f361-038f-45c7-ab6d-5326a0488e34"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" LR, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, CharLevel Vectors: 0.7048710601719198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odFGh1iEFgP4"
      },
      "source": [
        "### SVM Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCeidP4_FgQM",
        "outputId": "d3887a84-b92d-40cd-bb54-b7d141b12ae5"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" SVM, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, Count Vectors: 0.7020057306590258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFePpn7jFgQN",
        "outputId": "f336972b-5bd5-4dab-ce8c-f3a8b2212754"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" SVM, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, WordLevel TF-IDF: 0.7392550143266475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y33Gg9uJFgQN",
        "outputId": "78e3f14d-e732-4867-a125-53ffda1ebbd9"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" SVM, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, N-Gram Vectors: 0.663323782234957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoPXz5yHFgQO",
        "outputId": "2ba457c2-fad4-4e0c-a00d-c3d381307545"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" SVM, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, CharLevel Vectors: 0.7177650429799427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7TuyLjUGp3H"
      },
      "source": [
        "### Random Forrest Evaluation Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdCzAPlxGp3J",
        "outputId": "c14741f4-b5d2-4311-91a7-fc4b84b18b69"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" RF, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, Count Vectors: 0.6934097421203438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bb_LaiEGp3L",
        "outputId": "8f62ac1b-0a74-49be-bd27-b2451b824696"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.6905444126074498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKaPM_l4Gp3N",
        "outputId": "c74505c3-e122-4038-d770-b2f8f208213e"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" RF, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, N-Gram Vectors: 0.6532951289398281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz19N6xAGp3O",
        "outputId": "08f14f01-66b4-4ba4-b38f-2960135dc45f"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" RF, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, CharLevel Vectors: 0.6432664756446992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8GM_FdKdyu"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YlSs-eiKcxi",
        "outputId": "9811087d-8ec8-4e84-9b63-8dbf71676081"
      },
      "source": [
        "predictions = EXB.predict(xvalid_count.tocsc())\n",
        "print(\" EXB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " EXB, Count Vectors: 0.6977077363896849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPsEyHWLAUD",
        "outputId": "ccf787ce-4a67-4ada-d6f5-b5f696cde024"
      },
      "source": [
        "predictions = EXB.predict(xvalid_tfidf.tocsc())\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.6805157593123209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5QeBxTLccA"
      },
      "source": [
        "### Shallow NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-8D2HxLgdb",
        "outputId": "c62bb1bc-4c4f-48b3-ad8d-f05b67be9480"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" Shallow NN, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Shallow NN, WordLevel TF-IDF: 0.27793696275071633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eInI6X92DFhR"
      },
      "source": [
        "### Deep  --> poor results due to embeddings from different domain\n",
        "Need better pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMZt_DwiNNAa",
        "outputId": "76a6b281-7ced-4fc7-fe19-e50eee71244e"
      },
      "source": [
        "predictions = classifier.predict(valid_seq_x)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" CNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " CNN, WordEmbeddings: 0.27507163323782235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UUk0qsQn4vC",
        "outputId": "dcd2213e-38d4-4f5f-d829-c820d6dea36c"
      },
      "source": [
        "predictions = classifier_lstm.predict(valid_seq_x)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" LSTM, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LSTM, WordEmbeddings: 0.27507163323782235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyX78Op0orX5",
        "outputId": "ce223d38-a518-4d9a-b3b9-e0e25ca5b7f2"
      },
      "source": [
        "predictions = classifier_rnn.predict(valid_seq_x)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" RNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RNN, WordEmbeddings: 0.27507163323782235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf61SO40o6Oh",
        "outputId": "ec1691de-1e6c-473a-bc09-73491ebed1ae"
      },
      "source": [
        "predictions = classifier_brnn.predict(valid_seq_x)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" B RNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " B RNN, WordEmbeddings: 0.27507163323782235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKqWS0-0pmIw",
        "outputId": "80a70bbd-7754-4803-a101-c6d1f28ebc42"
      },
      "source": [
        "predictions = classifier_brnn.predict(valid_seq_x)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" RCNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RCNN, WordEmbeddings: 0.27507163323782235\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}