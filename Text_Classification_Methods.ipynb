{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "janWv1vG5xUD",
        "bQGOVFFwaepj",
        "fOKz8xQr5xXJ",
        "fBoBD2BsZVUW",
        "qYP5Sg-VaqEi",
        "bpAWfjkCl7oH",
        "sR62UJuynCDW",
        "8qY2Ug9KyTqN",
        "lKaOn7EUDAYi",
        "gWPOVg0pDMTK",
        "odFGh1iEFgP4",
        "w7TuyLjUGp3H",
        "Ka8GM_FdKdyu",
        "oU5QeBxTLccA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivangibithel/IRMiDis_Task2/blob/main/Text_Classification_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27hY5InNb60"
      },
      "source": [
        "[A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPQFpPa90fo"
      },
      "source": [
        "Models\n",
        "1. Naive Bayes Classifier\n",
        "2. Linear Classifier\n",
        "3. Support Vector Machine\n",
        "4. Bagging Models\n",
        "5. Boosting Models\n",
        "6. Shallow Neural Networks\n",
        "7. Deep Neural Networks\n",
        "    1. Convolutional Neural Network (CNN)\n",
        "    2. Long Short Term Modelr (LSTM)\n",
        "    3. Gated Recurrent Unit (GRU)\n",
        "    4. Bidirectional RNN\n",
        "    5. Recurrent Convolutional Neural Network (RCNN)\n",
        "    Other Variants of Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janWv1vG5xUD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PV3HrLnLJAY"
      },
      "source": [
        "# pip install emoji --upgrade"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvvarqE5xWm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd #to work with csv files\n",
        "\n",
        "#matplotlib imports are used to plot confusion matrices for the classifiers\n",
        "import matplotlib as mpl \n",
        "import matplotlib.cm as cm \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#import feature extraction methods from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "\n",
        "#pre-processing of text\n",
        "import string\n",
        "import re\n",
        "\n",
        "#import classifiers from sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#import different metrics to evaluate the classifiers\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn import metrics\n",
        "\n",
        "#import time function from time module to track the training duration\n",
        "from time import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFWAClI3mkMK",
        "outputId": "1c3a37d1-7a80-4727-ecd2-27dd72548561"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGOVFFwaepj"
      },
      "source": [
        "# Section 1: Load and explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iPh67KcCh5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "71f7b7f3-dd58-4b57-9eb3-8902975ed8f1"
      },
      "source": [
        "our_data = pd.read_csv(\"cleaned_train.csv\",index_col = \"id\")\n",
        "test_data = pd.read_csv(\"cleaned_test.csv\",index_col = \"id\")\n",
        "our_data = shuffle(our_data)\n",
        "our_data.head()"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.329110e+18</th>\n",
              "      <td>any government who proposes mandatory vaccines...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329210e+18</th>\n",
              "      <td>im so fucking excited for the coronavirus vacc...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329210e+18</th>\n",
              "      <td>the fastest way to end the #covid pandemic is ...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325790e+18</th>\n",
              "      <td>#covid vaccine first milestone vaccine offers ...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326080e+18</th>\n",
              "      <td>global stocks back to record highs as pfizer v...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet  ... label_num\n",
              "id                                                               ...          \n",
              "1.329110e+18  any government who proposes mandatory vaccines...  ...         1\n",
              "1.329210e+18  im so fucking excited for the coronavirus vacc...  ...         0\n",
              "1.329210e+18  the fastest way to end the #covid pandemic is ...  ...         0\n",
              "1.325790e+18  #covid vaccine first milestone vaccine offers ...  ...         0\n",
              "1.326080e+18  global stocks back to record highs as pfizer v...  ...         2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LbED8Q185xWu",
        "outputId": "e3e22633-e623-4258-a34f-8a385be5d2a8"
      },
      "source": [
        "display(our_data.shape) #Number of rows (instances) and columns in the dataset\n",
        "print(our_data[\"label\"].value_counts()/our_data.shape[0]) #Class distribution in the dataset"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2792, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Neutral    0.361748\n",
            "ProVax     0.354943\n",
            "AntiVax    0.283309\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYW_S3585xXF"
      },
      "source": [
        "# Prepare Dataset\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['tweet'] = our_data.tweet\n",
        "trainDF['label'] = our_data.label\n",
        "\n",
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['tweet'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "# train_y = encoder.fit_transform(trainDF['label'])\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NQzeWmeMsatO",
        "outputId": "d874401a-bc60-456a-c2dd-428743c34e60"
      },
      "source": [
        "trainDF"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.329110e+18</th>\n",
              "      <td>any government who proposes mandatory vaccines...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329210e+18</th>\n",
              "      <td>im so fucking excited for the coronavirus vacc...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329210e+18</th>\n",
              "      <td>the fastest way to end the #covid pandemic is ...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325790e+18</th>\n",
              "      <td>#covid vaccine first milestone vaccine offers ...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326080e+18</th>\n",
              "      <td>global stocks back to record highs as pfizer v...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326200e+18</th>\n",
              "      <td>garyblack the tremendous positive reaction to ...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325830e+18</th>\n",
              "      <td>hijodelcuervo orips from article the #moderna ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.331240e+18</th>\n",
              "      <td>moderna to supply million doses of its covid v...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326170e+18</th>\n",
              "      <td>great news #vaccine #covid #covidvaccine</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325820e+18</th>\n",
              "      <td>yesssss per aspera ad astra fantastic news is ...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2792 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet    label\n",
              "id                                                                      \n",
              "1.329110e+18  any government who proposes mandatory vaccines...  AntiVax\n",
              "1.329210e+18  im so fucking excited for the coronavirus vacc...   ProVax\n",
              "1.329210e+18  the fastest way to end the #covid pandemic is ...   ProVax\n",
              "1.325790e+18  #covid vaccine first milestone vaccine offers ...   ProVax\n",
              "1.326080e+18  global stocks back to record highs as pfizer v...  Neutral\n",
              "...                                                         ...      ...\n",
              "1.326200e+18  garyblack the tremendous positive reaction to ...  AntiVax\n",
              "1.325830e+18  hijodelcuervo orips from article the #moderna ...  Neutral\n",
              "1.331240e+18  moderna to supply million doses of its covid v...  Neutral\n",
              "1.326170e+18           great news #vaccine #covid #covidvaccine   ProVax\n",
              "1.325820e+18  yesssss per aspera ad astra fantastic news is ...   ProVax\n",
              "\n",
              "[2792 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOKz8xQr5xXJ"
      },
      "source": [
        "### Section 2: Text Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhC5TZuL5xXK"
      },
      "source": [
        "Typical steps involve tokenization, lower casing, removing, stop words, punctuation markers etc, and vectorization. Other processes such as stemming/lemmatization can also be performed. Here, we are performing the following steps: removing br tags, punctuation, numbers, and stopwords. While we are using sklearn's list of stopwords, there are several other stop word lists (e.g., from NLTK) or sometimes, custom stopword lists are needed depending on the task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZSHdHZ5xXL"
      },
      "source": [
        "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
        "def clean(doc): #doc is a string of text\n",
        "    # doc = doc.replace(\"@\", \" \")\n",
        "    # doc = doc.replace(\" \", \" \")\n",
        "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
        "    # doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
        "    #remove punctuation and numbers\n",
        "    return doc"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpVmfv8KKswH"
      },
      "source": [
        "import re\n",
        "def remove_emojis(data):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bI2QiCPIT0"
      },
      "source": [
        "def remove_url(data):\n",
        "  url = re.compile(\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\", re.UNICODE)\n",
        "  return re.sub(url, '', data)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi371W3mGgmm"
      },
      "source": [
        "# for i in range(len(our_data)):\n",
        "#   our_data.tweet.iloc[i] = remove_url(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = clean(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = remove_emojis(our_data.tweet.iloc[i])"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-Fd5XT2hHDCx",
        "outputId": "ec4b4aa4-be8c-42b1-fe27-cf163bf88838"
      },
      "source": [
        "our_data.head()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.329110e+18</th>\n",
              "      <td>any government who proposes mandatory vaccines...</td>\n",
              "      <td>AntiVax</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329210e+18</th>\n",
              "      <td>im so fucking excited for the coronavirus vacc...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.329210e+18</th>\n",
              "      <td>the fastest way to end the #covid pandemic is ...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.325790e+18</th>\n",
              "      <td>#covid vaccine first milestone vaccine offers ...</td>\n",
              "      <td>ProVax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.326080e+18</th>\n",
              "      <td>global stocks back to record highs as pfizer v...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          tweet  ... label_num\n",
              "id                                                               ...          \n",
              "1.329110e+18  any government who proposes mandatory vaccines...  ...         1\n",
              "1.329210e+18  im so fucking excited for the coronavirus vacc...  ...         0\n",
              "1.329210e+18  the fastest way to end the #covid pandemic is ...  ...         0\n",
              "1.325790e+18  #covid vaccine first milestone vaccine offers ...  ...         0\n",
              "1.326080e+18  global stocks back to record highs as pfizer v...  ...         2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CfVm42o5xXS"
      },
      "source": [
        "# Section 3: Modeling\n",
        "\n",
        "Now we are ready for the modelling. We are going to use algorithms from sklearn package. We will go through the following steps:\n",
        "\n",
        "1 Read train data    \n",
        "2 Extract features from the training data using CountVectorizer, which is a bag of words feature  implementation. We will use the pre-processing function above in conjunction with Count Vectorizer  \n",
        "3 Transform the test data into the same feature vector as the training data.  \n",
        "4 Train the classifier  \n",
        "5 Evaluate the classifier  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BscPlX2tZOPS"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKL1ucwxswDu",
        "outputId": "15d7bcc1-acb7-4deb-c345-24ed4af89290"
      },
      "source": [
        "X_train = train_x\n",
        "# X_train = trainDF['tweet']\n",
        "y_train =train_y\n",
        "print(X_train.shape, y_train.shape, valid_x.shape, valid_y.shape)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094,) (2094,) (698,) (698,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHC3O8EwuJ1Q",
        "outputId": "a3365d0f-f655-459a-fba6-d901bf525512"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "1.326600e+18    fighting stigma govt has a plan to ensure covi...\n",
              "1.335960e+18    mrna long the canadian government has doubled ...\n",
              "1.329440e+18    astrazeneca vaccine shows strong immune respon...\n",
              "1.325790e+18    do they think we are fucking stupid just days ...\n",
              "1.325800e+18    donaldjtrumpjr all pharmaceutical companies ar...\n",
              "                                      ...                        \n",
              "1.330910e+18    what are the potential #supplychain challenges...\n",
              "1.325920e+18    tomfole spduckworth camestewart i think a mand...\n",
              "1.326080e+18    me i refuse to take the mark of the beast covi...\n",
              "1.328060e+18    govmikedewine and we need rapid home frequent ...\n",
              "1.325840e+18    never related to a tweet so much in my life fu...\n",
              "Name: tweet, Length: 2094, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBoBD2BsZVUW"
      },
      "source": [
        " ### Count Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsUyIBUD5xZI"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['tweet'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "X_train_dtm =  count_vect.transform(X_train)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjNXiaiP91UA",
        "outputId": "2f100c1b-3b0f-4720-ea42-f597e88aa79e"
      },
      "source": [
        "print(X_train_dtm.shape, xvalid_count.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 7716) (698, 7716)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYP5Sg-VaqEi"
      },
      "source": [
        "### TF-IDF Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KgnBDQAamup",
        "outputId": "fc804e22-b2ed-4cdb-df36-8e32ddbf73a6"
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "print(xtrain_tfidf.shape, xvalid_tfidf.shape)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "print(xtrain_tfidf_ngram.shape, xvalid_tfidf_ngram.shape)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train)\n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
        "print(xtrain_tfidf_ngram_chars.shape, xvalid_tfidf_ngram_chars.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT4c02k7uTIl",
        "outputId": "fb2e354a-320d-4d4f-c085-9b0ca4290bdb"
      },
      "source": [
        "print(xtrain_tfidf_ngram)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4155)\t0.20712033129118507\n",
            "  (0, 4066)\t0.3042002761324843\n",
            "  (0, 2962)\t0.3233232635365891\n",
            "  (0, 1666)\t0.28507728872837956\n",
            "  (0, 995)\t0.20235232602790756\n",
            "  (0, 915)\t0.3560142264282778\n",
            "  (0, 180)\t0.336891239024173\n",
            "  (0, 178)\t0.2609851738370408\n",
            "  (0, 155)\t0.27561303864875997\n",
            "  (0, 75)\t0.3560142264282778\n",
            "  (0, 74)\t0.3560142264282778\n",
            "  (1, 4397)\t0.22733882991202398\n",
            "  (1, 4099)\t0.28984530578623174\n",
            "  (1, 4098)\t0.28984530578623174\n",
            "  (1, 4033)\t0.1561104810608021\n",
            "  (1, 3135)\t0.22453931344602923\n",
            "  (1, 3134)\t0.22192054068412514\n",
            "  (1, 3104)\t0.3062978206920625\n",
            "  (1, 2744)\t0.2972433483571516\n",
            "  (1, 2380)\t0.28984530578623174\n",
            "  (1, 2372)\t0.3062978206920625\n",
            "  (1, 2368)\t0.2430731509456039\n",
            "  (1, 1631)\t0.2972433483571516\n",
            "  (1, 1283)\t0.2835903499173156\n",
            "  (1, 952)\t0.2475863418046144\n",
            "  :\t:\n",
            "  (2091, 1699)\t0.23633778910428332\n",
            "  (2091, 921)\t0.05068763119162931\n",
            "  (2091, 729)\t0.17883143638795057\n",
            "  (2092, 4690)\t0.26161637661656845\n",
            "  (2092, 4318)\t0.2167005761515349\n",
            "  (2092, 4170)\t0.30690369765155834\n",
            "  (2092, 4155)\t0.19660198538385398\n",
            "  (2092, 4074)\t0.20006642935431587\n",
            "  (2092, 3821)\t0.30690369765155834\n",
            "  (2092, 3819)\t0.14479878099391114\n",
            "  (2092, 3732)\t0.2818508338671341\n",
            "  (2092, 2704)\t0.2758729042545538\n",
            "  (2092, 2690)\t0.16852399642555482\n",
            "  (2092, 2624)\t0.3379344910485628\n",
            "  (2092, 2623)\t0.3016307900388162\n",
            "  (2092, 848)\t0.26588322005341275\n",
            "  (2092, 457)\t0.24626343568675466\n",
            "  (2092, 358)\t0.2758729042545538\n",
            "  (2093, 4020)\t0.32605159198606726\n",
            "  (2093, 3819)\t0.19603788240040085\n",
            "  (2093, 3267)\t0.4235992266636036\n",
            "  (2093, 3105)\t0.4235992266636036\n",
            "  (2093, 2433)\t0.4083671211876842\n",
            "  (2093, 1909)\t0.3962045198594826\n",
            "  (2093, 1507)\t0.41550592191100233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs7PJfyRb5dH"
      },
      "source": [
        "### Word Embeddings  --> Not Working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW11l6hplDuI",
        "outputId": "fa40ec21-709f-4cb2-9e25-fbdcacec613f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37yUHD1UlTqz"
      },
      "source": [
        "%cd gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucorOg3FkQko"
      },
      "source": [
        "Can also use pre-train embeddings from Gensim library in future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEZqudNOb7S9"
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['tweet'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAWfjkCl7oH"
      },
      "source": [
        "### Text / NLP based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN761pknl-tU"
      },
      "source": [
        "trainDF = our_data\n",
        "trainDF['char_count'] = trainDF['tweet'].apply(len)\n",
        "trainDF['word_count'] = trainDF['tweet'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rtfX1omKsr"
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "trainDF['noun_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR62UJuynCDW"
      },
      "source": [
        "### Topic Models as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73aiWP0InCgB"
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(X_train_dtm)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCmJpXB0oiYh"
      },
      "source": [
        "topic_summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXKcs3N3aHwT"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qY2Ug9KyTqN"
      },
      "source": [
        "### Non Deep Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMrq29lY8Rj",
        "outputId": "4bfdf3ea-88bd-489d-c0ca-24db5d42cdfb"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "nb = MultinomialNB() #instantiate a Multinomial Naive Bayes model\n",
        "nb.fit(X_train_dtm, y_train)#train the count vectorizer model\n",
        "# nb.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# nb.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# nb.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v7pM9hB5xbA",
        "outputId": "b093bfd5-ef10-4153-ab12-9d28acded3ac"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "logreg = LogisticRegression(class_weight=\"balanced\",max_iter=1000) #instantiate a logistic regression model\n",
        "# logreg.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "logreg.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# logreg.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# logreg.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLKusAQ5xbf",
        "outputId": "44bdcf6a-5651-49d9-dc45-67b4ba80ffa9"
      },
      "source": [
        "# Linear SVM Classifier\n",
        "classifier = LinearSVC(class_weight='balanced') #instantiate a logistic regression model\n",
        "# classifier.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNPTNoq1puGY",
        "outputId": "efb02b5e-e289-4e4c-9eb7-f1a1802bed55"
      },
      "source": [
        "# Bagging Model\n",
        "classifier = ensemble.RandomForestClassifier()\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "# classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiszTNxCqBlI",
        "outputId": "f3b850ce-aea4-46fc-ba0d-fc25480c7040"
      },
      "source": [
        "# Extereme Gradient Boosting\n",
        "EXB = xgboost.XGBClassifier()\n",
        "EXB.fit(X_train_dtm.tocsc(), y_train)\n",
        "# EXB.fit(xtrain_tfidf.tocsc(), y_train)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59v5MNTDqh7b",
        "outputId": "3c29c74e-c197-4043-9ed5-b5ac013980dc"
      },
      "source": [
        "# Shallow Neural Network\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
        "classifier.fit(xtrain_tfidf.toarray(), y_train,epochs =1, shuffle = True)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 1s 4ms/step - loss: 0.5355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e12d53450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQFa-Q54yNxa"
      },
      "source": [
        "### Deep Neural Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryufmv7ktJtV"
      },
      "source": [
        "from tensorflow import keras\n",
        "maxlen= 70\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=maxlen)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EOJ6Ypb3Yte"
      },
      "source": [
        "train_y = keras.utils.to_categorical(train_y, num_classes=3)\n",
        "valid_y = keras.utils.to_categorical(valid_y, num_classes=3)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpb96BDRwid6",
        "outputId": "58a43776-ade0-44b3-b281-c044c48db4ab"
      },
      "source": [
        "# CNN\n",
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"tanh\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "    \n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "classifier.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# classifier.fit(x_train, train_y,batch_size=64, epochs=7)\n",
        "# train_seq_x -- > Word Embeddings required -->Later\n",
        "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "# print \"CNN, Word Embeddings\",  accuracy"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 3s 65ms/step - loss: 1.0906 - accuracy: 0.4069 - val_loss: 0.8926 - val_accuracy: 0.6504\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.8686 - accuracy: 0.6205 - val_loss: 0.7305 - val_accuracy: 0.6819\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 2s 53ms/step - loss: 0.7181 - accuracy: 0.7056 - val_loss: 0.6621 - val_accuracy: 0.7221\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.6190 - accuracy: 0.7366 - val_loss: 0.6233 - val_accuracy: 0.7264\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.6005 - accuracy: 0.7522 - val_loss: 0.5903 - val_accuracy: 0.7536\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4904 - accuracy: 0.8096 - val_loss: 0.5867 - val_accuracy: 0.7779\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4514 - accuracy: 0.8258 - val_loss: 0.5595 - val_accuracy: 0.7837\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.3832 - accuracy: 0.8565 - val_loss: 0.5615 - val_accuracy: 0.7779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe213972a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvWJpN84oRL5",
        "outputId": "fb4ac547-b907-4055-a282-9aca3ec12fb4"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_49\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_50 (InputLayer)        [(None, 70)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_49 (Embedding)     (None, 70, 300)           2340900   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_49 (Spatia (None, 70, 300)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 68, 100)           90100     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_18 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 2,436,203\n",
            "Trainable params: 95,303\n",
            "Non-trainable params: 2,340,900\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DxmpbSQxt4_",
        "outputId": "31221fb1-7b59-49bf-8c9b-f3b7bc665290"
      },
      "source": [
        "#  LSTM\n",
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n",
        "\n",
        "classifier_lstm = create_rnn_lstm()\n",
        "classifier_lstm.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 8s 170ms/step - loss: 1.0695 - accuracy: 0.3899 - val_loss: 0.8776 - val_accuracy: 0.5946\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 5s 151ms/step - loss: 0.8928 - accuracy: 0.5889 - val_loss: 0.7573 - val_accuracy: 0.6762\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 5s 148ms/step - loss: 0.7611 - accuracy: 0.6792 - val_loss: 0.7190 - val_accuracy: 0.6791\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 5s 148ms/step - loss: 0.7524 - accuracy: 0.6784 - val_loss: 0.6732 - val_accuracy: 0.7106\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 5s 150ms/step - loss: 0.7188 - accuracy: 0.7011 - val_loss: 0.6876 - val_accuracy: 0.7106\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 5s 150ms/step - loss: 0.6851 - accuracy: 0.6984 - val_loss: 0.7030 - val_accuracy: 0.7034\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 5s 150ms/step - loss: 0.6539 - accuracy: 0.7157 - val_loss: 0.6585 - val_accuracy: 0.7235\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 5s 149ms/step - loss: 0.6194 - accuracy: 0.7513 - val_loss: 0.6500 - val_accuracy: 0.7235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1d902d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21BAFxUnsEXW"
      },
      "source": [
        "classifier_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUjJbbzzycW_",
        "outputId": "7fe71692-4d38-473b-c715-863ce451dabf"
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_rnn = create_rnn_gru()\n",
        "classifier_rnn.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 7s 144ms/step - loss: 1.0893 - accuracy: 0.3674 - val_loss: 1.0382 - val_accuracy: 0.4327\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 1.0209 - accuracy: 0.4902 - val_loss: 0.8574 - val_accuracy: 0.5817\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 4s 123ms/step - loss: 0.8749 - accuracy: 0.5673 - val_loss: 0.8561 - val_accuracy: 0.5960\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 4s 124ms/step - loss: 0.8388 - accuracy: 0.6114 - val_loss: 0.8353 - val_accuracy: 0.6089\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 4s 124ms/step - loss: 0.8149 - accuracy: 0.6434 - val_loss: 0.7588 - val_accuracy: 0.6576\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 4s 126ms/step - loss: 0.7890 - accuracy: 0.6585 - val_loss: 0.7376 - val_accuracy: 0.6562\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 4s 126ms/step - loss: 0.7240 - accuracy: 0.6923 - val_loss: 0.6804 - val_accuracy: 0.7135\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.6532 - accuracy: 0.7347 - val_loss: 0.6754 - val_accuracy: 0.7264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1cd134910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWrlNY6O3GlI",
        "outputId": "fb78f19b-3c36-4794-f6a0-818ad0bde84a"
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_brnn = create_bidirectional_rnn()\n",
        "classifier_brnn.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 13s 272ms/step - loss: 1.0899 - accuracy: 0.3895 - val_loss: 1.0163 - val_accuracy: 0.4957\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 8s 232ms/step - loss: 0.9697 - accuracy: 0.5343 - val_loss: 0.9229 - val_accuracy: 0.5659\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 8s 230ms/step - loss: 0.8905 - accuracy: 0.5729 - val_loss: 0.8322 - val_accuracy: 0.6089\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 8s 233ms/step - loss: 0.8588 - accuracy: 0.5996 - val_loss: 0.7843 - val_accuracy: 0.6404\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 0.7753 - accuracy: 0.6518 - val_loss: 0.7320 - val_accuracy: 0.6963\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 7s 228ms/step - loss: 0.7564 - accuracy: 0.6710 - val_loss: 0.7161 - val_accuracy: 0.7020\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 0.6683 - accuracy: 0.7311 - val_loss: 0.6979 - val_accuracy: 0.7049\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 0.6404 - accuracy: 0.7277 - val_loss: 0.6876 - val_accuracy: 0.7063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1e54cc150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zNGS61S3dvn",
        "outputId": "17177293-a6cf-4d85-e884-e57512cfb68f"
      },
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"tanh\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_rcnn = create_rcnn()\n",
        "classifier_rcnn.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 3s 65ms/step - loss: 1.1101 - accuracy: 0.3801 - val_loss: 0.9261 - val_accuracy: 0.6003\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 2s 53ms/step - loss: 0.9175 - accuracy: 0.5809 - val_loss: 0.7658 - val_accuracy: 0.6576\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.7363 - accuracy: 0.6994 - val_loss: 0.6849 - val_accuracy: 0.7149\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.6436 - accuracy: 0.7277 - val_loss: 0.6300 - val_accuracy: 0.7450\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.5666 - accuracy: 0.7734 - val_loss: 0.6089 - val_accuracy: 0.7421\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.5035 - accuracy: 0.7911 - val_loss: 0.5885 - val_accuracy: 0.7579\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.4426 - accuracy: 0.8445 - val_loss: 0.5629 - val_accuracy: 0.7808\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.3766 - accuracy: 0.8767 - val_loss: 0.5697 - val_accuracy: 0.7736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1d52b0850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcMKcvRgaLOK"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKaOn7EUDAYi"
      },
      "source": [
        "### Naive Bayes Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6KeYb_t4pKw",
        "outputId": "17f00085-84c0-4c35-b3ab-33e6abd1e7fb"
      },
      "source": [
        "predictions = nb.predict(xvalid_count)\n",
        "print(\" NB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, Count Vectors: 0.7106017191977078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd9YSAvQxVRT",
        "outputId": "93e661f3-bb20-4749-e16d-36418de02df4"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 0, 2, 1, 0, 1, 2, 1, 2, 1, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0,\n",
              "       0, 0, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 1,\n",
              "       1, 0, 2, 1, 2, 2, 1, 2, 0, 1, 0, 2, 2, 2, 2, 0, 0, 2, 2, 1, 2, 2,\n",
              "       1, 2, 0, 2, 0, 1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0,\n",
              "       2, 1, 2, 1, 2, 1, 2, 0, 0, 2, 1, 2, 1, 0, 2, 2, 0, 2, 2, 1, 2, 2,\n",
              "       1, 1, 2, 1, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2,\n",
              "       2, 0, 2, 1, 0, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1,\n",
              "       1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 0, 1, 1, 0,\n",
              "       1, 1, 0, 2, 2, 0, 2, 1, 1, 2, 0, 2, 1, 0, 1, 0, 2, 2, 0, 2, 0, 0,\n",
              "       1, 2, 1, 1, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 0,\n",
              "       1, 2, 0, 2, 1, 2, 2, 0, 2, 0, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 2,\n",
              "       0, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 1, 1, 2, 2, 1, 2, 1, 0, 1, 2, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 1, 2, 2, 2, 0, 1, 2,\n",
              "       2, 0, 0, 1, 0, 1, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 1, 0, 2, 2, 2, 0,\n",
              "       2, 1, 1, 0, 2, 1, 2, 0, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 0, 0, 1, 0,\n",
              "       2, 2, 2, 2, 0, 2, 1, 2, 1, 0, 2, 2, 0, 1, 0, 2, 1, 0, 0, 0, 2, 1,\n",
              "       0, 1, 1, 2, 1, 0, 1, 2, 1, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0,\n",
              "       1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2, 1, 2, 0, 0, 0,\n",
              "       2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 1, 1, 0, 2, 0, 2, 1, 1, 0, 2, 2, 0,\n",
              "       0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 1, 1,\n",
              "       1, 0, 1, 0, 2, 2, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2,\n",
              "       0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 2, 0, 2, 1, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 0, 0, 0, 0, 1, 2,\n",
              "       1, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 0, 2, 0, 1, 2,\n",
              "       2, 1, 1, 2, 0, 2, 2, 0, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0,\n",
              "       2, 1, 2, 0, 2, 1, 0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 0, 1, 0, 0, 2,\n",
              "       2, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 0, 2, 1, 0, 2, 1, 1, 2, 1,\n",
              "       2, 2, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 2, 0, 1, 1, 2,\n",
              "       1, 1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 0, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1,\n",
              "       2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2,\n",
              "       2, 2, 2, 1, 2, 0, 1, 2, 0, 2, 1, 0, 2, 2, 2, 0, 1, 2, 1, 1, 0, 1,\n",
              "       0, 2, 0, 1, 0, 0, 2, 2, 1, 0, 0, 2, 1, 0, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhxvc2rCBEw",
        "outputId": "83bbb7f4-51a8-4de2-97c0-781cfb378395"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf)\n",
        "print(\" NB, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, WordLevel TF-IDF: 0.7134670487106017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxBrixtvCI7J",
        "outputId": "86ed6ec9-a422-4f23-8151-9c3fddb4f5e1"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram)\n",
        "print(\" NB, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, N-Gram Vectors: 0.6762177650429799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVHjLFMgCJo_",
        "outputId": "e664b356-509f-4319-9ac8-3083c9ffa701"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" NB, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, CharLevel Vectors: 0.673352435530086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPOVg0pDMTK"
      },
      "source": [
        "### Linear Classification --> Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2RGbV5EpX5",
        "outputId": "45d38b65-9527-488f-9727-5a93741bd826"
      },
      "source": [
        "predictions = logreg.predict(xvalid_count)\n",
        "print(\" LR, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, Count Vectors: 0.7134670487106017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdyB1-njEpX9",
        "outputId": "694cd428-5fe4-4ef7-d183-075fc8c5d1ba"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf)\n",
        "print(\" LR, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, WordLevel TF-IDF: 0.7306590257879656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0BO3ZGaEpX-",
        "outputId": "f4a5e660-2ea7-4fe3-d2db-2719e339fda1"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram)\n",
        "print(\" LR, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, N-Gram Vectors: 0.6776504297994269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNyApNB0EpYA",
        "outputId": "4e58f361-038f-45c7-ab6d-5326a0488e34"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" LR, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, CharLevel Vectors: 0.7048710601719198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odFGh1iEFgP4"
      },
      "source": [
        "### SVM Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCeidP4_FgQM",
        "outputId": "d3887a84-b92d-40cd-bb54-b7d141b12ae5"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" SVM, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, Count Vectors: 0.7020057306590258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFePpn7jFgQN",
        "outputId": "f336972b-5bd5-4dab-ce8c-f3a8b2212754"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" SVM, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, WordLevel TF-IDF: 0.7392550143266475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y33Gg9uJFgQN",
        "outputId": "78e3f14d-e732-4867-a125-53ffda1ebbd9"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" SVM, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, N-Gram Vectors: 0.663323782234957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoPXz5yHFgQO",
        "outputId": "2ba457c2-fad4-4e0c-a00d-c3d381307545"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" SVM, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, CharLevel Vectors: 0.7177650429799427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7TuyLjUGp3H"
      },
      "source": [
        "### Random Forrest Evaluation Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdCzAPlxGp3J",
        "outputId": "c14741f4-b5d2-4311-91a7-fc4b84b18b69"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" RF, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, Count Vectors: 0.6934097421203438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bb_LaiEGp3L",
        "outputId": "8f62ac1b-0a74-49be-bd27-b2451b824696"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.6905444126074498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKaPM_l4Gp3N",
        "outputId": "c74505c3-e122-4038-d770-b2f8f208213e"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" RF, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, N-Gram Vectors: 0.6532951289398281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz19N6xAGp3O",
        "outputId": "08f14f01-66b4-4ba4-b38f-2960135dc45f"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" RF, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, CharLevel Vectors: 0.6432664756446992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8GM_FdKdyu"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YlSs-eiKcxi",
        "outputId": "9811087d-8ec8-4e84-9b63-8dbf71676081"
      },
      "source": [
        "predictions = EXB.predict(xvalid_count.tocsc())\n",
        "print(\" EXB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " EXB, Count Vectors: 0.6977077363896849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPsEyHWLAUD",
        "outputId": "ccf787ce-4a67-4ada-d6f5-b5f696cde024"
      },
      "source": [
        "predictions = EXB.predict(xvalid_tfidf.tocsc())\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.6805157593123209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5QeBxTLccA"
      },
      "source": [
        "### Shallow NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-8D2HxLgdb",
        "outputId": "c62bb1bc-4c4f-48b3-ad8d-f05b67be9480"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" Shallow NN, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Shallow NN, WordLevel TF-IDF: 0.27793696275071633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eInI6X92DFhR"
      },
      "source": [
        "### Deep  --> poor results due to embeddings from different domain\n",
        "Need better pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMZt_DwiNNAa"
      },
      "source": [
        "# predictions = classifier.predict(x_val)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" CNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UUk0qsQn4vC"
      },
      "source": [
        "# predictions = classifier_lstm.predict(valid_seq_x)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" LSTM, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyX78Op0orX5"
      },
      "source": [
        "# predictions = classifier_rnn.predict(x_val)\n",
        "# # predictions = predictions.argmax(axis=-1)\n",
        "# print(\" RNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf61SO40o6Oh"
      },
      "source": [
        "# predictions = classifier_brnn.predict(valid_seq_x)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" B RNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKqWS0-0pmIw"
      },
      "source": [
        "# predictions = classifier_rcnn.predict(valid_seq_x)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" RCNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}