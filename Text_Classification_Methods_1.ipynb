{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "janWv1vG5xUD",
        "bQGOVFFwaepj",
        "fOKz8xQr5xXJ",
        "fBoBD2BsZVUW",
        "qYP5Sg-VaqEi",
        "bpAWfjkCl7oH",
        "sR62UJuynCDW",
        "8qY2Ug9KyTqN",
        "lKaOn7EUDAYi",
        "gWPOVg0pDMTK",
        "odFGh1iEFgP4",
        "w7TuyLjUGp3H",
        "Ka8GM_FdKdyu",
        "oU5QeBxTLccA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivangibithel/IRMiDis_Task2/blob/main/Text_Classification_Methods_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27hY5InNb60"
      },
      "source": [
        "[A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPQFpPa90fo"
      },
      "source": [
        "Models\n",
        "1. Naive Bayes Classifier\n",
        "2. Linear Classifier\n",
        "3. Support Vector Machine\n",
        "4. Bagging Models\n",
        "5. Boosting Models\n",
        "6. Shallow Neural Networks\n",
        "7. Deep Neural Networks\n",
        "    1. Convolutional Neural Network (CNN)\n",
        "    2. Long Short Term Modelr (LSTM)\n",
        "    3. Gated Recurrent Unit (GRU)\n",
        "    4. Bidirectional RNN\n",
        "    5. Recurrent Convolutional Neural Network (RCNN)\n",
        "    Other Variants of Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janWv1vG5xUD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PV3HrLnLJAY"
      },
      "source": [
        "# pip install emoji --upgrade"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvvarqE5xWm",
        "outputId": "805078ec-f100-4a8c-ee09-aab5aef89d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd #to work with csv files\n",
        "\n",
        "#matplotlib imports are used to plot confusion matrices for the classifiers\n",
        "import matplotlib as mpl \n",
        "import matplotlib.cm as cm \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#import feature extraction methods from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "\n",
        "#pre-processing of text\n",
        "import string\n",
        "import re\n",
        "\n",
        "#import classifiers from sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#import different metrics to evaluate the classifiers\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn import metrics\n",
        "\n",
        "#import time function from time module to track the training duration\n",
        "from time import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC16rHfcpHeI",
        "outputId": "fa846137-81a6-47c5-b9c5-171682549df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "import re\n",
        "!pip install emoji --quiet\n",
        "import emoji\n",
        "!pip install contractions --quiet\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import unicodedata\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 133kB 31.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 286kB 35.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 46.9MB/s \n",
            "\u001b[?25h  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFWAClI3mkMK",
        "outputId": "1090a742-c43c-46a6-b08e-c6bdc9f148a5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGOVFFwaepj"
      },
      "source": [
        "# Section 1: Load and explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iPh67KcCh5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "3e344ddc-2bb4-497a-fb9c-9edc2304b2fc"
      },
      "source": [
        "# our_data = pd.read_csv(\"irmidis-2021-task2-train.csv\")\n",
        "# test_data = pd.read_csv(\"irmidis-2021-task2-test.csv\")\n",
        "our_data = pd.read_csv(\"cleaned_dataset_train.csv\",index_col = \"id\")\n",
        "test_data = pd.read_csv(\"cleaned_dataset_test.csv\",index_col = \"id\")\n",
        "# our_data = shuffle(our_data)\n",
        "our_data.head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1325682517148569600</th>\n",
              "      <td>0</td>\n",
              "      <td>coronavirus  some canadians hesitant to take a...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325684180483600384</th>\n",
              "      <td>1</td>\n",
              "      <td>moderna on track to report late stage covid 19...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325689685943996416</th>\n",
              "      <td>2</td>\n",
              "      <td>the philippine government is in talks with ang...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325690517724782593</th>\n",
              "      <td>3</td>\n",
              "      <td>care homes to be first to receive coronavirus ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325697646909132800</th>\n",
              "      <td>4</td>\n",
              "      <td>this is why thinking that a vaccine is a panac...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Unnamed: 0  ...    label\n",
              "id                               ...         \n",
              "1325682517148569600           0  ...  AntiVax\n",
              "1325684180483600384           1  ...  Neutral\n",
              "1325689685943996416           2  ...  Neutral\n",
              "1325690517724782593           3  ...  Neutral\n",
              "1325697646909132800           4  ...  AntiVax\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "LbED8Q185xWu",
        "outputId": "78bc36f9-2456-4ded-bc14-c3bf9120c6ac"
      },
      "source": [
        "display(our_data.shape) #Number of rows (instances) and columns in the dataset\n",
        "print(our_data[\"label\"].value_counts()/our_data.shape[0]) #Class distribution in the dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2792, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Neutral    0.361748\n",
            "ProVax     0.354943\n",
            "AntiVax    0.283309\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYW_S3585xXF"
      },
      "source": [
        "# Prepare Dataset\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['tweet'] = our_data.tweet\n",
        "trainDF['label'] = our_data.label\n",
        "\n",
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['tweet'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "# train_y = encoder.fit_transform(trainDF['label'])\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "NQzeWmeMsatO",
        "outputId": "c40c1608-42a1-4f4b-dbf0-7bb88c1e274e"
      },
      "source": [
        "trainDF"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1325682517148569600</th>\n",
              "      <td>coronavirus  some canadians hesitant to take a...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325684180483600384</th>\n",
              "      <td>moderna on track to report late stage covid 19...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325689685943996416</th>\n",
              "      <td>the philippine government is in talks with ang...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325690517724782593</th>\n",
              "      <td>care homes to be first to receive coronavirus ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325697646909132800</th>\n",
              "      <td>this is why thinking that a vaccine is a panac...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325972470336278530</th>\n",
              "      <td>if a covid 19 vaccine is effective it will be ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325770248382001152</th>\n",
              "      <td>pfizer claims coronavirus vaccine success  pla...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325784714062409728</th>\n",
              "      <td>pump dat  covid19 vaccine  flexed biceps  face...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325992175218417665</th>\n",
              "      <td>lol  not a chance i would get any vaccine let ...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333154694198398978</th>\n",
              "      <td>michellemalkin  article titled avoiding pitfa...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2792 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                 tweet    label\n",
              "id                                                                             \n",
              "1325682517148569600  coronavirus  some canadians hesitant to take a...  AntiVax\n",
              "1325684180483600384  moderna on track to report late stage covid 19...  Neutral\n",
              "1325689685943996416  the philippine government is in talks with ang...  Neutral\n",
              "1325690517724782593  care homes to be first to receive coronavirus ...  Neutral\n",
              "1325697646909132800  this is why thinking that a vaccine is a panac...  AntiVax\n",
              "...                                                                ...      ...\n",
              "1325972470336278530  if a covid 19 vaccine is effective it will be ...  Neutral\n",
              "1325770248382001152  pfizer claims coronavirus vaccine success  pla...  Neutral\n",
              "1325784714062409728  pump dat  covid19 vaccine  flexed biceps  face...   ProVax\n",
              "1325992175218417665  lol  not a chance i would get any vaccine let ...  AntiVax\n",
              "1333154694198398978   michellemalkin  article titled avoiding pitfa...  AntiVax\n",
              "\n",
              "[2792 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOKz8xQr5xXJ"
      },
      "source": [
        "### Section 2: Text Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhC5TZuL5xXK"
      },
      "source": [
        "Typical steps involve tokenization, lower casing, removing, stop words, punctuation markers etc, and vectorization. Other processes such as stemming/lemmatization can also be performed. Here, we are performing the following steps: removing br tags, punctuation, numbers, and stopwords. While we are using sklearn's list of stopwords, there are several other stop word lists (e.g., from NLTK) or sometimes, custom stopword lists are needed depending on the task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_hJLiMooE1B",
        "outputId": "36930b62-b5db-4f74-92bf-4e04f236f7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "# def cleaning(text):\n",
        "#   text= text.lower()\n",
        "#   text= emoji.demojize(text)\n",
        "#   text=contractions.fix(text)\n",
        "#   text=text.strip()\n",
        "#   text=text.replace('[^\\w\\s]','')\n",
        "#   text=re.sub(r'http\\S+', '', text)\n",
        "#   REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "#   BAD_SYMBOLS_RE = re.compile('[^0-9a-z +]')\n",
        "#   text = REPLACE_BY_SPACE_RE.sub(' ' , text)\n",
        "#   text = BAD_SYMBOLS_RE.sub(' ',text)\n",
        "  \n",
        "#   return text\n",
        "\n",
        "# clean=trainDF['tweet'].apply(cleaning)\n",
        "# # STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# ff=[]\n",
        "# for i in clean:\n",
        "#   text=unicodedata.normalize('NFKD', i).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "#   ff.append(text)\n",
        "# dd=pd.DataFrame(ff)\n",
        "# dataset = pd.concat([trainDF['id'],dd,trainDF['label']],axis=1)\n",
        "# dataset.columns = [\"id\",\"tweet\",\"label\"]\n",
        "# dataset"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1325682517148569600</td>\n",
              "      <td>coronavirus  some canadians hesitant to take a...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1325684180483600384</td>\n",
              "      <td>moderna on track to report late stage covid 19...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1325689685943996416</td>\n",
              "      <td>the philippine government is in talks with ang...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1325690517724782593</td>\n",
              "      <td>care homes to be first to receive coronavirus ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1325697646909132800</td>\n",
              "      <td>this is why thinking that a vaccine is a panac...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2787</th>\n",
              "      <td>1325972470336278530</td>\n",
              "      <td>if a covid 19 vaccine is effective it will be ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2788</th>\n",
              "      <td>1325770248382001152</td>\n",
              "      <td>pfizer claims coronavirus vaccine success  pla...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2789</th>\n",
              "      <td>1325784714062409728</td>\n",
              "      <td>pump dat  covid19 vaccine  flexed biceps  face...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2790</th>\n",
              "      <td>1325992175218417665</td>\n",
              "      <td>lol  not a chance i would get any vaccine let ...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791</th>\n",
              "      <td>1333154694198398978</td>\n",
              "      <td>michellemalkin  article titled avoiding pitfa...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2792 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  ...    label\n",
              "0     1325682517148569600  ...  AntiVax\n",
              "1     1325684180483600384  ...  Neutral\n",
              "2     1325689685943996416  ...  Neutral\n",
              "3     1325690517724782593  ...  Neutral\n",
              "4     1325697646909132800  ...  AntiVax\n",
              "...                   ...  ...      ...\n",
              "2787  1325972470336278530  ...  Neutral\n",
              "2788  1325770248382001152  ...  Neutral\n",
              "2789  1325784714062409728  ...   ProVax\n",
              "2790  1325992175218417665  ...  AntiVax\n",
              "2791  1333154694198398978  ...  AntiVax\n",
              "\n",
              "[2792 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_qDkwD_ozzK",
        "outputId": "3cd700b3-f4ee-4399-8ecc-f97858a0f0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "# clean_test=test_data['tweet'].apply(cleaning)\n",
        "# ff=[]\n",
        "# for i in clean_test:\n",
        "#   text=unicodedata.normalize('NFKD', i).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "#   ff.append(text)\n",
        "# dd=pd.DataFrame(ff)\n",
        "# dataset_test = pd.concat([test_data['id'],dd],axis=1)\n",
        "# dataset_test.columns = [\"id\",\"tweet\"]\n",
        "# dataset_test"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1259097719341297664</td>\n",
              "      <td>polls say millions of americans will refuse co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1307596484251062273</td>\n",
              "      <td>you are right  one should also remember that t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1296241188823953409</td>\n",
              "      <td>so     i should have got the keys to my new ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1294639481216045056</td>\n",
              "      <td>cdc   july 2020 acip meeting   overview of cov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1319741954721239041</td>\n",
              "      <td>notably absent is a vaccine for the nightmare ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>1275394676594540547</td>\n",
              "      <td>unlocked  face with medical mask  syringe  vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>1339072387363508224</td>\n",
              "      <td>aakobel  ethanhovermale  dolmenlord  strictly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>1328165880660955142</td>\n",
              "      <td>charliekirk11 it is been 9 months since covid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>1337209455986020353</td>\n",
              "      <td>apparently the covid 19 vaccine caused bell s ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>1338143437921533954</td>\n",
              "      <td>r14scorae  dirtroadliving1 so pfizer does not...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id                                              tweet\n",
              "0     1259097719341297664  polls say millions of americans will refuse co...\n",
              "1     1307596484251062273  you are right  one should also remember that t...\n",
              "2     1296241188823953409  so     i should have got the keys to my new ho...\n",
              "3     1294639481216045056  cdc   july 2020 acip meeting   overview of cov...\n",
              "4     1319741954721239041  notably absent is a vaccine for the nightmare ...\n",
              "...                   ...                                                ...\n",
              "1595  1275394676594540547   unlocked  face with medical mask  syringe  vi...\n",
              "1596  1339072387363508224   aakobel  ethanhovermale  dolmenlord  strictly...\n",
              "1597  1328165880660955142   charliekirk11 it is been 9 months since covid...\n",
              "1598  1337209455986020353  apparently the covid 19 vaccine caused bell s ...\n",
              "1599  1338143437921533954   r14scorae  dirtroadliving1 so pfizer does not...\n",
              "\n",
              "[1600 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO2S996troFD",
        "outputId": "491d6dc5-db51-4759-e294-019658cb8ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "# trainDF = dataset\n",
        "# trainDF"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1325682517148569600</td>\n",
              "      <td>coronavirus  some canadians hesitant to take a...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1325684180483600384</td>\n",
              "      <td>moderna on track to report late stage covid 19...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1325689685943996416</td>\n",
              "      <td>the philippine government is in talks with ang...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1325690517724782593</td>\n",
              "      <td>care homes to be first to receive coronavirus ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1325697646909132800</td>\n",
              "      <td>this is why thinking that a vaccine is a panac...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2787</th>\n",
              "      <td>1325972470336278530</td>\n",
              "      <td>if a covid 19 vaccine is effective it will be ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2788</th>\n",
              "      <td>1325770248382001152</td>\n",
              "      <td>pfizer claims coronavirus vaccine success  pla...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2789</th>\n",
              "      <td>1325784714062409728</td>\n",
              "      <td>pump dat  covid19 vaccine  flexed biceps  face...</td>\n",
              "      <td>ProVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2790</th>\n",
              "      <td>1325992175218417665</td>\n",
              "      <td>lol  not a chance i would get any vaccine let ...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791</th>\n",
              "      <td>1333154694198398978</td>\n",
              "      <td>michellemalkin  article titled avoiding pitfa...</td>\n",
              "      <td>AntiVax</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2792 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  ...    label\n",
              "0     1325682517148569600  ...  AntiVax\n",
              "1     1325684180483600384  ...  Neutral\n",
              "2     1325689685943996416  ...  Neutral\n",
              "3     1325690517724782593  ...  Neutral\n",
              "4     1325697646909132800  ...  AntiVax\n",
              "...                   ...  ...      ...\n",
              "2787  1325972470336278530  ...  Neutral\n",
              "2788  1325770248382001152  ...  Neutral\n",
              "2789  1325784714062409728  ...   ProVax\n",
              "2790  1325992175218417665  ...  AntiVax\n",
              "2791  1333154694198398978  ...  AntiVax\n",
              "\n",
              "[2792 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtXGro_UzMn1"
      },
      "source": [
        "# trainDF.to_csv(\"cleaned_dataset_train.csv\")\n",
        "# dataset_test.to_csv(\"cleaned_dataset_test.csv\")"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6MN_PaXrQQ_"
      },
      "source": [
        "### commented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZSHdHZ5xXL"
      },
      "source": [
        "# stopwords = stop_words.ENGLISH_STOP_WORDS\n",
        "# def clean(doc): #doc is a string of text\n",
        "#     # doc = doc.replace(\"@\", \" \")\n",
        "#     # doc = doc.replace(\" \", \" \")\n",
        "#     doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
        "#     # doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
        "#     #remove punctuation and numbers\n",
        "#     return doc"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpVmfv8KKswH"
      },
      "source": [
        "# import re\n",
        "# def remove_emojis(data):\n",
        "#     emoj = re.compile(\"[\"\n",
        "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#         u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "#         u\"\\U00002702-\\U000027B0\"\n",
        "#         u\"\\U00002702-\\U000027B0\"\n",
        "#         u\"\\U000024C2-\\U0001F251\"\n",
        "#         u\"\\U0001f926-\\U0001f937\"\n",
        "#         u\"\\U00010000-\\U0010ffff\"\n",
        "#         u\"\\u2640-\\u2642\" \n",
        "#         u\"\\u2600-\\u2B55\"\n",
        "#         u\"\\u200d\"\n",
        "#         u\"\\u23cf\"\n",
        "#         u\"\\u23e9\"\n",
        "#         u\"\\u231a\"\n",
        "#         u\"\\ufe0f\"  # dingbats\n",
        "#         u\"\\u3030\"\n",
        "#                       \"]+\", re.UNICODE)\n",
        "#     return re.sub(emoj, '', data)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bI2QiCPIT0"
      },
      "source": [
        "# def remove_url(data):\n",
        "#   url = re.compile(\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\", re.UNICODE)\n",
        "#   return re.sub(url, '', data)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi371W3mGgmm"
      },
      "source": [
        "# for i in range(len(our_data)):\n",
        "#   our_data.tweet.iloc[i] = remove_url(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = clean(our_data.tweet.iloc[i])\n",
        "#   our_data.tweet.iloc[i] = remove_emojis(our_data.tweet.iloc[i])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fd5XT2hHDCx"
      },
      "source": [
        "# our_data.head()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CfVm42o5xXS"
      },
      "source": [
        "# Section 3: Modeling\n",
        "\n",
        "Now we are ready for the modelling. We are going to use algorithms from sklearn package. We will go through the following steps:\n",
        "\n",
        "1 Read train data    \n",
        "2 Extract features from the training data using CountVectorizer, which is a bag of words feature  implementation. We will use the pre-processing function above in conjunction with Count Vectorizer  \n",
        "3 Transform the test data into the same feature vector as the training data.  \n",
        "4 Train the classifier  \n",
        "5 Evaluate the classifier  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BscPlX2tZOPS"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKL1ucwxswDu",
        "outputId": "0c0a6034-d6f3-412c-caae-5b7102fd0630"
      },
      "source": [
        "X_train = train_x\n",
        "# X_train = trainDF['tweet']\n",
        "y_train =train_y\n",
        "print(X_train.shape, y_train.shape, valid_x.shape, valid_y.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094,) (2094,) (698,) (698,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHC3O8EwuJ1Q",
        "outputId": "448f2486-cc6b-43d2-bf27-1872b57ad1ce"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "1335040955490193408    biden urges taking coronavirus vaccine  wearin...\n",
              "1330276381780025346    sorry to disappoint those retweeting this gloo...\n",
              "1334847647925088257     albanian government pays 3 9 million dollars ...\n",
              "1333838050133159937    the government is not proposing to make a covi...\n",
              "1336113459180744705    well a  tory that talks senses   matthanock  b...\n",
              "                                             ...                        \n",
              "1327332997222297600     chelseaclinton  realdonaldtrump who is your d...\n",
              "1326174223212154880    some good news in addition to the vaccine tria...\n",
              "1325779532255784961    as a    layman  would someone please explain e...\n",
              "1325672362029834240    kids are participating in covid 19 vaccine tri...\n",
              "1325931184342396928    a safe  effective covid 19 vaccination is fina...\n",
              "Name: tweet, Length: 2094, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBoBD2BsZVUW"
      },
      "source": [
        " ### Count Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsUyIBUD5xZI"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['tweet'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "X_train_dtm =  count_vect.transform(X_train)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjNXiaiP91UA",
        "outputId": "c1f614b2-be5b-46a0-d47f-73ecccd9a784"
      },
      "source": [
        "print(X_train_dtm.shape, xvalid_count.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 7913) (698, 7913)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYP5Sg-VaqEi"
      },
      "source": [
        "### TF-IDF Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KgnBDQAamup",
        "outputId": "dc71a54b-9b07-4329-cec4-f4e5c8b1ae4e"
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "print(xtrain_tfidf.shape, xvalid_tfidf.shape)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "print(xtrain_tfidf_ngram.shape, xvalid_tfidf_ngram.shape)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['tweet'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train)\n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
        "print(xtrain_tfidf_ngram_chars.shape, xvalid_tfidf_ngram_chars.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n",
            "(2094, 5000) (698, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT4c02k7uTIl",
        "outputId": "97a360b4-88e3-4be4-d5a3-2bd5a95c3962"
      },
      "source": [
        "print(xtrain_tfidf_ngram)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4717)\t0.4337950763867642\n",
            "  (0, 3921)\t0.4337950763867642\n",
            "  (0, 3324)\t0.4057409908675666\n",
            "  (0, 3323)\t0.4003403161927118\n",
            "  (0, 2623)\t0.2874276917197443\n",
            "  (0, 1002)\t0.2030829524926654\n",
            "  (0, 727)\t0.418196340736321\n",
            "  (1, 4354)\t0.1574970934766017\n",
            "  (1, 3351)\t0.36260663635478096\n",
            "  (1, 2553)\t0.45132837651731544\n",
            "  (1, 2552)\t0.45132837651731544\n",
            "  (1, 2517)\t0.19099285644584413\n",
            "  (1, 2516)\t0.16460603118893888\n",
            "  (1, 2331)\t0.21737968170274938\n",
            "  (1, 2213)\t0.16134245543534126\n",
            "  (1, 2199)\t0.2077444503941093\n",
            "  (1, 2116)\t0.1255614528344626\n",
            "  (1, 2080)\t0.21737968170274938\n",
            "  (1, 1805)\t0.16347657514575695\n",
            "  (1, 1652)\t0.10712461965069046\n",
            "  (1, 1055)\t0.07038520064016571\n",
            "  (1, 1042)\t0.0586361052446266\n",
            "  (1, 300)\t0.21095371918789332\n",
            "  (1, 299)\t0.17250910915282597\n",
            "  (1, 202)\t0.12116475104186661\n",
            "  :\t:\n",
            "  (2093, 4556)\t0.16946510934012424\n",
            "  (2093, 4311)\t0.24500039914662508\n",
            "  (2093, 3927)\t0.21216737374153435\n",
            "  (2093, 3926)\t0.17193134596114057\n",
            "  (2093, 3773)\t0.23775794038180914\n",
            "  (2093, 3772)\t0.20674538277409915\n",
            "  (2093, 3712)\t0.2152608086314722\n",
            "  (2093, 3615)\t0.22250326739628817\n",
            "  (2093, 3479)\t0.16434902544788393\n",
            "  (2093, 3264)\t0.2152608086314722\n",
            "  (2093, 3263)\t0.2021008301837418\n",
            "  (2093, 2974)\t0.19118011889774486\n",
            "  (2093, 2852)\t0.21868044225116426\n",
            "  (2093, 2848)\t0.1389433966322551\n",
            "  (2093, 2708)\t0.14542213779821928\n",
            "  (2093, 2670)\t0.20934328894855775\n",
            "  (2093, 2667)\t0.1507413640347041\n",
            "  (2093, 2227)\t0.23184042069889468\n",
            "  (2093, 1267)\t0.19618331050082735\n",
            "  (2093, 1266)\t0.19618331050082735\n",
            "  (2093, 1053)\t0.19442847771147748\n",
            "  (2093, 1042)\t0.06608653153233196\n",
            "  (2093, 589)\t0.19803847258308357\n",
            "  (2093, 259)\t0.20000613564595124\n",
            "  (2093, 34)\t0.19442847771147748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs7PJfyRb5dH"
      },
      "source": [
        "### Word Embeddings  --> Not Working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW11l6hplDuI",
        "outputId": "7692c254-c9a4-435a-ec94-ebc779a1ca85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37yUHD1UlTqz",
        "outputId": "3d95664d-3965-4fcf-b716-05898fd87cf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd gdrive/MyDrive/"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucorOg3FkQko"
      },
      "source": [
        "Can also use pre-train embeddings from Gensim library in future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEZqudNOb7S9"
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['tweet'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAWfjkCl7oH"
      },
      "source": [
        "### Text / NLP based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN761pknl-tU"
      },
      "source": [
        "trainDF = our_data\n",
        "trainDF['char_count'] = trainDF['tweet'].apply(len)\n",
        "trainDF['word_count'] = trainDF['tweet'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rtfX1omKsr"
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "trainDF['noun_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['tweet'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVoh1ieeqohd"
      },
      "source": [
        "trainDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR62UJuynCDW"
      },
      "source": [
        "### Topic Models as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73aiWP0InCgB"
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(X_train_dtm)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCmJpXB0oiYh"
      },
      "source": [
        "topic_summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXKcs3N3aHwT"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qY2Ug9KyTqN"
      },
      "source": [
        "### Non Deep Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMrq29lY8Rj",
        "outputId": "ce15336e-6a6f-4094-9311-d4c5066d72e9"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "nb = MultinomialNB() #instantiate a Multinomial Naive Bayes model\n",
        "# nb.fit(X_train_dtm, y_train)#train the count vectorizer model\n",
        "nb.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# nb.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# nb.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v7pM9hB5xbA",
        "outputId": "f46ab97b-4a02-4037-cbcb-1d7cd6c56b5c"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "logreg = LogisticRegression(class_weight=\"balanced\",max_iter=1000) #instantiate a logistic regression model\n",
        "# logreg.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "logreg.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# logreg.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# logreg.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLKusAQ5xbf",
        "outputId": "fb67c3b5-9721-452b-d950-d99c8502985e"
      },
      "source": [
        "# Linear SVM Classifier\n",
        "classifier = LinearSVC(class_weight='balanced') #instantiate a logistic regression model\n",
        "# classifier.fit(X_train_dtm, y_train) #fit the model with training data\n",
        "classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNPTNoq1puGY",
        "outputId": "d907afdc-c86e-41fb-baeb-0b55fdc85925"
      },
      "source": [
        "# Bagging Model\n",
        "classifier = ensemble.RandomForestClassifier()\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "# classifier.fit(xtrain_tfidf, y_train)#train the TF-IDF model\n",
        "# classifier.fit(xtrain_tfidf_ngram, y_train)#train the TF-IDF ngram model\n",
        "# classifier.fit(xtrain_tfidf_ngram_chars, y_train)#train the TF-IDF ngram CHAR model"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiszTNxCqBlI",
        "outputId": "bd4da165-5b5e-48a8-fbc8-86883bf6d65e"
      },
      "source": [
        "# Extereme Gradient Boosting\n",
        "EXB = xgboost.XGBClassifier()\n",
        "# EXB.fit(X_train_dtm.tocsc(), y_train)\n",
        "EXB.fit(xtrain_tfidf.tocsc(), y_train)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59v5MNTDqh7b",
        "outputId": "bf88561f-e0f5-458f-a818-c16769f4fe22"
      },
      "source": [
        "# Shallow Neural Network\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"tanh\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(3, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(70)\n",
        "classifier.fit(x_train, train_y,batch_size=64, epochs=2, validation_data=(x_val, valid_y))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "33/33 [==============================] - 1s 12ms/step - loss: 1.5233 - accuracy: 0.3538 - val_loss: 1.3762 - val_accuracy: 0.3625\n",
            "Epoch 2/2\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3155 - accuracy: 0.3966 - val_loss: 1.2990 - val_accuracy: 0.3782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8e150d2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQFa-Q54yNxa"
      },
      "source": [
        "### Deep Neural Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryufmv7ktJtV"
      },
      "source": [
        "from tensorflow import keras\n",
        "maxlen= 70\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=maxlen)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EOJ6Ypb3Yte"
      },
      "source": [
        "train_y = keras.utils.to_categorical(train_y, num_classes=3)\n",
        "valid_y = keras.utils.to_categorical(valid_y, num_classes=3)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpb96BDRwid6",
        "outputId": "b4a6df8d-6deb-4f0b-bebc-51624e8a4c8a"
      },
      "source": [
        "# CNN\n",
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"tanh\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "    \n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "classifier.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# classifier.fit(x_train, train_y,batch_size=64, epochs=7)\n",
        "# train_seq_x -- > Word Embeddings required -->Later\n",
        "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "# print \"CNN, Word Embeddings\",  accuracy"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 4s 61ms/step - loss: 1.0586 - accuracy: 0.4325 - val_loss: 0.8934 - val_accuracy: 0.5745\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 2s 51ms/step - loss: 0.8557 - accuracy: 0.6196 - val_loss: 0.7543 - val_accuracy: 0.6705\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 2s 51ms/step - loss: 0.7158 - accuracy: 0.7040 - val_loss: 0.6819 - val_accuracy: 0.6905\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 2s 50ms/step - loss: 0.6026 - accuracy: 0.7583 - val_loss: 0.6464 - val_accuracy: 0.7335\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 2s 50ms/step - loss: 0.5598 - accuracy: 0.7639 - val_loss: 0.6011 - val_accuracy: 0.7407\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 2s 51ms/step - loss: 0.4669 - accuracy: 0.8259 - val_loss: 0.5836 - val_accuracy: 0.7464\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 2s 51ms/step - loss: 0.4317 - accuracy: 0.8478 - val_loss: 0.6013 - val_accuracy: 0.7579\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 2s 51ms/step - loss: 0.3738 - accuracy: 0.8659 - val_loss: 0.5765 - val_accuracy: 0.7493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8e03e8210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvWJpN84oRL5",
        "outputId": "fb4ac547-b907-4055-a282-9aca3ec12fb4"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_49\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_50 (InputLayer)        [(None, 70)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_49 (Embedding)     (None, 70, 300)           2340900   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_49 (Spatia (None, 70, 300)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 68, 100)           90100     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_18 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 2,436,203\n",
            "Trainable params: 95,303\n",
            "Non-trainable params: 2,340,900\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DxmpbSQxt4_",
        "outputId": "e12747a2-1414-4620-b0b1-7e3c7f78e71b"
      },
      "source": [
        "#  LSTM\n",
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n",
        "\n",
        "classifier_lstm = create_rnn_lstm()\n",
        "classifier_lstm.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 9s 199ms/step - loss: 1.0866 - accuracy: 0.3715 - val_loss: 0.9645 - val_accuracy: 0.5215\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 5s 143ms/step - loss: 0.9430 - accuracy: 0.5042 - val_loss: 0.8794 - val_accuracy: 0.5444\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 5s 143ms/step - loss: 0.8377 - accuracy: 0.5962 - val_loss: 0.7945 - val_accuracy: 0.6275\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 5s 145ms/step - loss: 0.7883 - accuracy: 0.6447 - val_loss: 0.7483 - val_accuracy: 0.6705\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 5s 144ms/step - loss: 0.7693 - accuracy: 0.6646 - val_loss: 0.7059 - val_accuracy: 0.6991\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 5s 144ms/step - loss: 0.6496 - accuracy: 0.7365 - val_loss: 0.7271 - val_accuracy: 0.6691\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 5s 144ms/step - loss: 0.6786 - accuracy: 0.7128 - val_loss: 0.7031 - val_accuracy: 0.7092\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 5s 143ms/step - loss: 0.6389 - accuracy: 0.7499 - val_loss: 0.7174 - val_accuracy: 0.6934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8ddacdc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21BAFxUnsEXW"
      },
      "source": [
        "classifier_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUjJbbzzycW_",
        "outputId": "86c3ed43-8c4a-4edb-d5b7-451c81d6b6cf"
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_rnn = create_rnn_gru()\n",
        "classifier_rnn.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 8s 138ms/step - loss: 1.0920 - accuracy: 0.3670 - val_loss: 1.0458 - val_accuracy: 0.4355\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 4s 117ms/step - loss: 1.0171 - accuracy: 0.4473 - val_loss: 0.9205 - val_accuracy: 0.5473\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 4s 118ms/step - loss: 0.8942 - accuracy: 0.5525 - val_loss: 0.8334 - val_accuracy: 0.6103\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 4s 119ms/step - loss: 0.8151 - accuracy: 0.6180 - val_loss: 0.8023 - val_accuracy: 0.6103\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 4s 117ms/step - loss: 0.7662 - accuracy: 0.6432 - val_loss: 0.7879 - val_accuracy: 0.6261\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 4s 117ms/step - loss: 0.7687 - accuracy: 0.6455 - val_loss: 0.7347 - val_accuracy: 0.6719\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 4s 117ms/step - loss: 0.6754 - accuracy: 0.7218 - val_loss: 0.7351 - val_accuracy: 0.6648\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 4s 118ms/step - loss: 0.6781 - accuracy: 0.7182 - val_loss: 0.6788 - val_accuracy: 0.6977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8dc456310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWrlNY6O3GlI",
        "outputId": "fb78f19b-3c36-4794-f6a0-818ad0bde84a"
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_brnn = create_bidirectional_rnn()\n",
        "classifier_brnn.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 13s 272ms/step - loss: 1.0899 - accuracy: 0.3895 - val_loss: 1.0163 - val_accuracy: 0.4957\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 8s 232ms/step - loss: 0.9697 - accuracy: 0.5343 - val_loss: 0.9229 - val_accuracy: 0.5659\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 8s 230ms/step - loss: 0.8905 - accuracy: 0.5729 - val_loss: 0.8322 - val_accuracy: 0.6089\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 8s 233ms/step - loss: 0.8588 - accuracy: 0.5996 - val_loss: 0.7843 - val_accuracy: 0.6404\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 0.7753 - accuracy: 0.6518 - val_loss: 0.7320 - val_accuracy: 0.6963\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 7s 228ms/step - loss: 0.7564 - accuracy: 0.6710 - val_loss: 0.7161 - val_accuracy: 0.7020\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 0.6683 - accuracy: 0.7311 - val_loss: 0.6979 - val_accuracy: 0.7049\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 0.6404 - accuracy: 0.7277 - val_loss: 0.6876 - val_accuracy: 0.7063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1e54cc150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zNGS61S3dvn",
        "outputId": "17177293-a6cf-4d85-e884-e57512cfb68f"
      },
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"tanh\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"tanh\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier_rcnn = create_rcnn()\n",
        "classifier_rcnn.fit(x_train, train_y,batch_size=64, epochs=8, validation_data=(x_val, valid_y))\n",
        "# train_seq_x -- > Word Embeddings required -->Later"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "33/33 [==============================] - 3s 65ms/step - loss: 1.1101 - accuracy: 0.3801 - val_loss: 0.9261 - val_accuracy: 0.6003\n",
            "Epoch 2/8\n",
            "33/33 [==============================] - 2s 53ms/step - loss: 0.9175 - accuracy: 0.5809 - val_loss: 0.7658 - val_accuracy: 0.6576\n",
            "Epoch 3/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.7363 - accuracy: 0.6994 - val_loss: 0.6849 - val_accuracy: 0.7149\n",
            "Epoch 4/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.6436 - accuracy: 0.7277 - val_loss: 0.6300 - val_accuracy: 0.7450\n",
            "Epoch 5/8\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.5666 - accuracy: 0.7734 - val_loss: 0.6089 - val_accuracy: 0.7421\n",
            "Epoch 6/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.5035 - accuracy: 0.7911 - val_loss: 0.5885 - val_accuracy: 0.7579\n",
            "Epoch 7/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.4426 - accuracy: 0.8445 - val_loss: 0.5629 - val_accuracy: 0.7808\n",
            "Epoch 8/8\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.3766 - accuracy: 0.8767 - val_loss: 0.5697 - val_accuracy: 0.7736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1d52b0850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcMKcvRgaLOK"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKaOn7EUDAYi"
      },
      "source": [
        "### Naive Bayes Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6KeYb_t4pKw",
        "outputId": "5cc901da-20f3-4228-8eea-65463c682901"
      },
      "source": [
        "predictions = nb.predict(xvalid_count)\n",
        "print(\" NB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, Count Vectors: 0.7263610315186246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd9YSAvQxVRT"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhxvc2rCBEw",
        "outputId": "ea7f1203-cdd6-4eb5-a1e4-f3a9ac5f5812"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf)\n",
        "print(\" NB, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, WordLevel TF-IDF: 0.7306590257879656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxBrixtvCI7J",
        "outputId": "2c8397a5-cdb7-437d-dbd7-492ab1bb67e7"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram)\n",
        "print(\" NB, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, N-Gram Vectors: 0.7005730659025788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVHjLFMgCJo_",
        "outputId": "05945f73-a314-42ba-f5ea-cfa84ec8038a"
      },
      "source": [
        "predictions = nb.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" NB, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " NB, CharLevel Vectors: 0.6891117478510028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPOVg0pDMTK"
      },
      "source": [
        "### Linear Classification --> Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2RGbV5EpX5",
        "outputId": "7ed5bda4-0687-4d37-88e8-8113f875939c"
      },
      "source": [
        "predictions = logreg.predict(xvalid_count)\n",
        "print(\" LR, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, Count Vectors: 0.7378223495702005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdyB1-njEpX9",
        "outputId": "3831ba45-a700-48a3-8f9a-ef71714e6a6b"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf)\n",
        "print(\" LR, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, WordLevel TF-IDF: 0.7392550143266475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0BO3ZGaEpX-",
        "outputId": "644a026d-37c4-4b5e-86ac-91d1737e6a98"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram)\n",
        "print(\" LR, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, N-Gram Vectors: 0.7106017191977078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNyApNB0EpYA",
        "outputId": "764f7ad5-c51c-4c89-8c22-cd260084c3ad"
      },
      "source": [
        "predictions = logreg.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" LR, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LR, CharLevel Vectors: 0.7249283667621776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odFGh1iEFgP4"
      },
      "source": [
        "### SVM Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCeidP4_FgQM",
        "outputId": "7d65870b-31fc-4036-cce7-93652a6f78fa"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" SVM, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, Count Vectors: 0.7048710601719198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFePpn7jFgQN",
        "outputId": "3a992a85-c683-41f4-b7d0-109bf0e4d49c"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" SVM, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, WordLevel TF-IDF: 0.7335243553008596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y33Gg9uJFgQN",
        "outputId": "dcf338ee-5a8b-4350-8b36-9cbf8bb70f84"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" SVM, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, N-Gram Vectors: 0.7034383954154728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoPXz5yHFgQO",
        "outputId": "29d04bf7-c58a-4954-accb-08fdcad133e2"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" SVM, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM, CharLevel Vectors: 0.7091690544412608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7TuyLjUGp3H"
      },
      "source": [
        "### Random Forrest Evaluation Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdCzAPlxGp3J",
        "outputId": "16315042-07e2-4965-dcf0-7e655a99af3e"
      },
      "source": [
        "predictions = classifier.predict(xvalid_count)\n",
        "print(\" RF, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, Count Vectors: 0.7148997134670487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bb_LaiEGp3L",
        "outputId": "6bb22fac-eeb5-45bc-c7d3-48173999d552"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf)\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.7020057306590258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKaPM_l4Gp3N",
        "outputId": "f07713a3-e467-4b8d-d014-3a7c6c67d984"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram)\n",
        "print(\" RF, N-Gram Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, N-Gram Vectors: 0.6547277936962751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz19N6xAGp3O",
        "outputId": "11738141-3e92-49e8-e549-36d4e08288a5"
      },
      "source": [
        "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
        "print(\" RF, CharLevel Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, CharLevel Vectors: 0.6833810888252149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8GM_FdKdyu"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YlSs-eiKcxi",
        "outputId": "78141c5e-8e91-4e8d-e50e-f8d8b0a66e5d"
      },
      "source": [
        "predictions = EXB.predict(xvalid_count.tocsc())\n",
        "print(\" EXB, Count Vectors:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " EXB, Count Vectors: 0.6991404011461319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPsEyHWLAUD",
        "outputId": "0a2b16ec-ebc8-4a65-eb7d-d3289905fe07"
      },
      "source": [
        "predictions = EXB.predict(xvalid_tfidf.tocsc())\n",
        "print(\" RF, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " RF, WordLevel TF-IDF: 0.7106017191977078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5QeBxTLccA"
      },
      "source": [
        "### Shallow NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "Fc-8D2HxLgdb",
        "outputId": "5a172a83-fb96-4a26-8b9a-ea361b805d84"
      },
      "source": [
        "predictions = classifier.predict(x_val)\n",
        "predictions = predictions.argmax(axis=-1)\n",
        "print(\" Shallow NN, WordLevel TF-IDF:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-c75efb51f46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Shallow NN, WordLevel TF-IDF:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and multilabel-indicator targets"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eInI6X92DFhR"
      },
      "source": [
        "### Deep  --> poor results due to embeddings from different domain\n",
        "Need better pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMZt_DwiNNAa"
      },
      "source": [
        "# predictions = classifier.predict(x_val)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" CNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UUk0qsQn4vC"
      },
      "source": [
        "# predictions = classifier_lstm.predict(valid_seq_x)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" LSTM, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyX78Op0orX5"
      },
      "source": [
        "# predictions = classifier_rnn.predict(x_val)\n",
        "# # predictions = predictions.argmax(axis=-1)\n",
        "# print(\" RNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf61SO40o6Oh"
      },
      "source": [
        "# predictions = classifier_brnn.predict(valid_seq_x)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" B RNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKqWS0-0pmIw"
      },
      "source": [
        "# predictions = classifier_rcnn.predict(valid_seq_x)\n",
        "# predictions = predictions.argmax(axis=-1)\n",
        "# print(\" RCNN, WordEmbeddings:\", metrics.accuracy_score(predictions, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}